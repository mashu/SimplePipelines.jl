var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Types","page":"API Reference","title":"Types","text":"","category":"section"},{"location":"api/#Macros","page":"API Reference","title":"Macros","text":"","category":"section"},{"location":"api/#Operators","page":"API Reference","title":"Operators","text":"","category":"section"},{"location":"api/#Functions","page":"API Reference","title":"Functions","text":"","category":"section"},{"location":"api/#Shell-helpers","page":"API Reference","title":"Shell helpers","text":"","category":"section"},{"location":"api/#Execution","page":"API Reference","title":"Execution","text":"","category":"section"},{"location":"api/#Utilities","page":"API Reference","title":"Utilities","text":"","category":"section"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/#SimplePipelines.AbstractNode","page":"API Reference","title":"SimplePipelines.AbstractNode","text":"AbstractNode\n\nBase type for all pipeline nodes. Subtypes are:\n\nStep: A single unit of work\nSequence: Nodes that run in order\nParallel: Nodes that run concurrently\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Step","page":"API Reference","title":"SimplePipelines.Step","text":"Step{F}\n\nA single unit of work in the pipeline.\n\nType Parameters\n\nF: The type of work (e.g., Cmd for shell commands, or a Function subtype)\n\nFields\n\nname::Symbol: Identifier for the step\nwork::F: The work to execute\ninputs::Vector{String}: Input file dependencies (optional)\noutputs::Vector{String}: Output files produced (optional)\n\nExamples\n\n# Shell command\nStep(:align, `bwa mem ref.fa reads.fq`)\n\n# Julia function\nStep(:qc, () -> run_quality_control())\n\n# With file dependencies\nStep(:variant_call, `bcftools call -m aligned.bam`, [\"aligned.bam\"], [\"variants.vcf\"])\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Sequence","page":"API Reference","title":"SimplePipelines.Sequence","text":"Sequence{T<:Tuple}\n\nA sequence of nodes that execute in order. The type parameter T captures the exact tuple type for full type stability.\n\nExamples\n\n# Created via >> operator\nalign >> sort >> index\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Parallel","page":"API Reference","title":"SimplePipelines.Parallel","text":"Parallel{T<:Tuple}\n\nNodes that execute concurrently. The type parameter T captures the exact tuple type for full type stability.\n\nExamples\n\n# Created via & operator\n(sample_a & sample_b & sample_c) >> merge\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Retry","page":"API Reference","title":"SimplePipelines.Retry","text":"Retry{N<:AbstractNode}\n\nRetry a node up to max_attempts times on failure, with optional delay between attempts.\n\nExamples\n\n# Retry up to 3 times\nRetry(flaky_step, 3)\n\n# Retry with delay between attempts\nRetry(api_call, 5, delay=2.0)\n\n# Using | operator for simple fallback (try once, then fallback)\nrisky_step | safe_fallback\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Fallback","page":"API Reference","title":"SimplePipelines.Fallback","text":"Fallback{A<:AbstractNode, B<:AbstractNode}\n\nTry the primary node; if it fails, run the fallback node. Created with the | operator.\n\nExamples\n\n# If fast_method fails, use slow_method\nfast_method | slow_method\n\n# Chain multiple fallbacks\nmethod_a | method_b | method_c\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Branch","page":"API Reference","title":"SimplePipelines.Branch","text":"Branch{C<:Function, T<:AbstractNode, F<:AbstractNode}\n\nConditional execution: run if_true when condition() returns true, otherwise if_false.\n\nExamples\n\n# Branch based on file size\nBranch(() -> filesize(\"data.txt\") > 1_000_000, large_file_pipeline, small_file_pipeline)\n\n# Branch based on environment\nBranch(() -> haskey(ENV, \"DEBUG\"), debug_steps, normal_steps)\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Timeout","page":"API Reference","title":"SimplePipelines.Timeout","text":"Timeout{N<:AbstractNode}\n\nFail if a node doesn't complete within the specified time.\n\nExamples\n\n# 30 second timeout\nTimeout(long_step, 30.0)\n\n# Combine with retry and fallback\nRetry(Timeout(primary, 10.0), 3) | fallback\n\n# Using ^ operator: step^3 = retry 3 times\nTimeout(api_call, 5.0)^3 | backup\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Reduce","page":"API Reference","title":"SimplePipelines.Reduce","text":"Reduce{F<:Function, N<:AbstractNode}\n\nRun a node (typically Parallel) and combine outputs with a reducing function. The function receives a vector of outputs from successful steps.\n\nExamples\n\n# Combine parallel outputs\nReduce(a & b & c) do outputs\n    join(outputs, \"\\n\")\nend\n\n# With a named function\nReduce(combine_results, process_a & process_b)\n\n# In a pipeline\nfetch >> Reduce(merge, analyze_a & analyze_b) >> report\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Pipeline","page":"API Reference","title":"SimplePipelines.Pipeline","text":"Pipeline{N<:AbstractNode}\n\nA complete pipeline ready for execution.\n\nFields\n\nroot::N: The root node of the pipeline DAG\nname::String: Human-readable name for the pipeline\n\nExamples\n\n# Create from composed nodes\np = Pipeline(align >> sort >> index, name=\"alignment\")\n\n# Or wrap multiple nodes (creates a Sequence)\np = Pipeline(step1, step2, step3)\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.@step","page":"API Reference","title":"SimplePipelines.@step","text":"@step expr\n@step name = expr\n@step name(inputs => outputs) = expr\n\nCreate a Step with optional name and file dependencies.\n\nExamples\n\n# Anonymous step\n@step `fastqc sample.fq`\n\n# Named step\n@step align = `bwa mem ref.fa reads.fq > aligned.sam`\n\n# With file dependencies (for dependency tracking)\n@step sort(\"aligned.sam\" => \"sorted.bam\") = `samtools sort aligned.sam`\n\n# Julia function\n@step qc_report = generate_multiqc_report\n\n\n\n\n\n","category":"macro"},{"location":"api/#Base.:>>","page":"API Reference","title":"Base.:>>","text":"a >> b\n\nCreate a Sequence where a completes before b starts.\n\nSupports chaining: a >> b >> c creates a single flattened sequence.\n\nExamples\n\n# Basic sequence\nfastqc >> trim >> align\n\n# Chain shell commands directly\n`fastqc raw.fq` >> `trimmomatic ...` >> `bwa mem ...`\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.:&","page":"API Reference","title":"Base.:&","text":"a & b\n\nCreate a Parallel where a and b run concurrently.\n\nSupports chaining: a & b & c creates a single parallel group.\n\nExamples\n\n# Process multiple samples in parallel\n(sample1 & sample2 & sample3) >> merge_results\n\n# Mix with sequences for complex DAGs\n(trim_a >> align_a) & (trim_b >> align_b) >> joint_call\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.:|","page":"API Reference","title":"Base.:|","text":"a | b\n\nCreate a Fallback: if a fails, run b.\n\nExamples\n\n# Try primary, fallback to secondary on failure\nprimary_method | fallback_method\n\n# Chain multiple fallbacks\nfast | medium | slow\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.:^","page":"API Reference","title":"Base.:^","text":"a^n\n\nCreate a Retry that attempts a up to n times.\n\nExamples\n\n# Retry step up to 3 times\nflaky_step^3\n\n# Combine with fallback\nflaky_step^3 | backup\n\n# With timeout\nTimeout(api_call, 5.0)^3\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.Map","page":"API Reference","title":"SimplePipelines.Map","text":"Map(f, items) -> Parallel\n\nApply function f to each item, creating parallel steps.\n\nExamples\n\n# Process files in parallel\nMap([\"a.txt\", \"b.txt\", \"c.txt\"]) do file\n    @step Symbol(file) = `process $file`\nend\n\n# With named steps\nsamples = [\"sample_A\", \"sample_B\", \"sample_C\"]\nMap(samples) do s\n    Step(Symbol(\"process_\", s), `analyze $s.fastq`)\nend >> merge_step\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.ForEach","page":"API Reference","title":"SimplePipelines.ForEach","text":"ForEach(pattern::String) do wildcard...\n    # commands using wildcard\nend\n\nDiscover files matching pattern with {name} placeholders and create a parallel branch for each match. Extracted values are passed to your function.\n\nExamples\n\n# With interpolation use sh(\"...\") for shell commands\nForEach(\"data/{sample}_R1.fq.gz\") do sample\n    sh(\"pear -f $(sample)_R1 -r $(sample)_R2\") >> sh(\"process $(sample)\")\nend\n\n# Multiple wildcards\nForEach(\"data/{project}/{sample}.csv\") do project, sample\n    sh(\"process $(project)/$(sample).csv\")\nend\n\n# Chain with downstream step\nForEach(\"{id}.fastq\") do id\n    sh(\"align $(id).fastq\")\nend >> @step merge = sh\"merge *.bam\"\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.@sh_str","page":"API Reference","title":"SimplePipelines.@sh_str","text":"sh\"command\"\nsh(command::String)\n\nShell commands: use sh\"...\" for literals, sh(\"...\") when you need interpolation. Julia's backticks cmd don't allow >, |, etc., so we use the short sh form (two characters) and wrap in sh -c \"...\".\n\nExamples\n\nsh\"sort data.txt > sorted.txt\"           # literal\nsh(\"pear -f $(sample)_R1 -r $(sample)_R2\")  # with interpolation\n\n\n\n\n\n","category":"macro"},{"location":"api/#SimplePipelines.sh","page":"API Reference","title":"SimplePipelines.sh","text":"Run a shell command string (with interpolation). Use sh(\"cmd $(var)\") in loops or when building commands. \n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.execute","page":"API Reference","title":"SimplePipelines.execute","text":"execute(step::Step{Cmd}) -> StepResult\n\nExecute a shell command step. Captures stdout/stderr.\n\n\n\n\n\nexecute(step::Step{F}) -> StepResult where F<:Function\n\nExecute a Julia function step. Captures the return value as string.\n\n\n\n\n\nexecute(p::Pipeline; verbose=true, dry_run=false)\nexecute(node::AbstractNode; verbose=true, dry_run=false)\n\nExecute a pipeline or node.\n\nArguments\n\nverbose::Bool=true: Print progress information\ndry_run::Bool=false: Show DAG structure without executing\n\nReturns\n\nVector{StepResult}: Results from all executed steps\n\nExamples\n\n# Run with progress output\nresults = execute(pipeline)\n\n# Silent execution\nresults = execute(pipeline, verbose=false)\n\n# Preview structure\nexecute(pipeline, dry_run=true)\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.count_steps","page":"API Reference","title":"SimplePipelines.count_steps","text":"count_steps(node::AbstractNode) -> Int\n\nCount total steps in a pipeline node.\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.steps","page":"API Reference","title":"SimplePipelines.steps","text":"steps(node::AbstractNode) -> Vector{Step}\n\nFlatten all steps from a pipeline node into a vector.\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.print_dag","page":"API Reference","title":"SimplePipelines.print_dag","text":"print_dag(node; indent=0)\n\nPrint the DAG structure of a pipeline node.\n\nExamples\n\npipeline = (a & b) >> c >> (d & e)\nprint_dag(pipeline)\n# Output:\n# Sequence:\n#   Parallel:\n#     a\n#     b\n#   c\n#   Parallel:\n#     d\n#     e\n\n\n\n\n\n","category":"function"},{"location":"design/#Design","page":"Design","title":"Design","text":"","category":"section"},{"location":"design/#Interface-Overview","page":"Design","title":"Interface Overview","text":"┌─────────────────────────────────────────────────────────────┐\n│                    SimplePipelines.jl                       │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  @step name = sh\"cmd\"       Shell step                      │\n│  @step name = sh\"cmd > f\"   Shell with redirection/pipes    │\n│  @step name = () -> ...     Julia step                      │\n│                                                             │\n│  a >> b                     Sequential: a then b            │\n│  a & b                      Parallel: a and b together      │\n│                                                             │\n│  execute(p)            Execute the pipeline            │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘","category":"section"},{"location":"design/#Type-Hierarchy","page":"Design","title":"Type Hierarchy","text":"AbstractNode\n    │\n    ├── Step{F}           Single unit of work\n    │                     F = Cmd | Function\n    │\n    ├── Sequence{T}       Sequential execution\n    │                     T = Tuple of nodes\n    │\n    └── Parallel{T}       Concurrent execution\n                          T = Tuple of nodes\n\nAll types are fully parametric—the compiler knows exact types at every level.","category":"section"},{"location":"design/#Composition-Model","page":"Design","title":"Composition Model","text":"# User writes:\nsh\"echo a\" >> (sh\"echo b\" & sh\"echo c\") >> sh\"echo d\"\n\n# Becomes:\nSequence{Tuple{\n    Step{Cmd},\n    Parallel{Tuple{Step{Cmd}, Step{Cmd}}},\n    Step{Cmd}\n}}\n\nThe complete structure is encoded in the type, enabling full compile-time specialization.","category":"section"},{"location":"design/#Execution-Flow","page":"Design","title":"Execution Flow","text":"execute(Pipeline)\n       │\n       ▼\nrun_node(root, verbosity)  ─── dispatches on node type\n       │\n       ├─► Step:     execute(step) → StepResult\n       │\n       ├─► Sequence: run first node, then recurse on rest\n       │\n       └─► Parallel: @spawn all nodes, fetch all results","category":"section"},{"location":"design/#Key-Design-Decisions","page":"Design","title":"Key Design Decisions","text":"","category":"section"},{"location":"design/#1.-Tuples,-Not-Vectors","page":"Design","title":"1. Tuples, Not Vectors","text":"# ✗ Vector: type information lost\nSequence(nodes::Vector{AbstractNode})\n\n# ✓ Tuple: exact types preserved\nSequence{Tuple{Step{Cmd}, Step{Function}}}\n\nTuples enable the compiler to generate specialized code for each node.","category":"section"},{"location":"design/#2.-Multiple-Dispatch,-Not-Type-Checks","page":"Design","title":"2. Multiple Dispatch, Not Type Checks","text":"# ✗ Runtime type checking (type unstable)\nfunction run_node(node)\n    if node isa Step\n        # ...\n    elseif node isa Sequence\n        # ...\n    end\nend\n\n# ✓ Multiple dispatch (type stable)\nrun_node(step::Step, v) = execute(step)\nrun_node(seq::Sequence, v) = _run_sequence!([], seq.nodes, v)\nrun_node(par::Parallel, v) = _spawn_parallel(par.nodes, v)","category":"section"},{"location":"design/#3.-Tuple-Recursion","page":"Design","title":"3. Tuple Recursion","text":"Iterate tuples in a type-stable way:\n\n# Base case\n_run_sequence!(results, ::Tuple{}, v) = nothing\n\n# Recursive case\nfunction _run_sequence!(results, nodes::Tuple, v)\n    append!(results, run_node(first(nodes), v))\n    _run_sequence!(results, Base.tail(nodes), v)\nend\n\nThe compiler unrolls this into efficient, specialized code.","category":"section"},{"location":"design/#4.-Verbosity-as-Types","page":"Design","title":"4. Verbosity as Types","text":"struct Verbose end\nstruct Silent end\n\nprint_start(::Silent, ::Step) = nothing\nprint_start(::Verbose, s::Step) = println(\"▶ $(s.name)\")\n\nDead code elimination removes printing when verbose=false.","category":"section"},{"location":"design/#Performance-Characteristics","page":"Design","title":"Performance Characteristics","text":"Aspect Design Choice Benefit\nNode storage Tuples Full type info, inline storage\nDispatch Multiple dispatch Zero runtime type checks\nIteration Recursion Compiler unrolling\nOperators @inline Zero call overhead\nVerbosity Singleton types Dead code elimination","category":"section"},{"location":"development/#Extending-SimplePipelines","page":"Development","title":"Extending SimplePipelines","text":"SimplePipelines is designed for easy extension via Julia's multiple dispatch.","category":"section"},{"location":"development/#Architecture","page":"Development","title":"Architecture","text":"All nodes inherit from AbstractNode. To add custom behavior, define a new subtype and implement dispatch methods:\n\nAbstractNode\n    ├── Step{F}           # Leaf node\n    ├── Sequence{T}       # Sequential\n    ├── Parallel{T}       # Concurrent\n    ├── Retry{N}          # Retry on failure\n    ├── Fallback{A,B}     # Try A, else B\n    └── Branch{C,T,F}     # Conditional","category":"section"},{"location":"development/#Custom-Node-in-4-Steps","page":"Development","title":"Custom Node in 4 Steps","text":"using SimplePipelines\nimport SimplePipelines: AbstractNode, run_node, print_dag, count_steps, steps\n\n# 1. Define type (parametric for type stability)\nstruct Timeout{N<:AbstractNode} <: AbstractNode\n    node::N\n    seconds::Float64\nend\n\n# 2. Implement execution\nfunction run_node(t::Timeout, verbosity)\n    # ... timeout logic ...\n    return run_node(t.node, verbosity)\nend\n\n# 3. Implement visualization\nprint_dag(t::Timeout, indent::Int) = begin\n    println(\"  \"^indent, \"Timeout($(t.seconds)s):\")\n    print_dag(t.node, indent + 1)\nend\n\n# 4. Implement utilities\ncount_steps(t::Timeout) = count_steps(t.node)\nsteps(t::Timeout) = steps(t.node)","category":"section"},{"location":"development/#Custom-Operator","page":"Development","title":"Custom Operator","text":"import Base: ^\n\n# Node^3 means repeat 3 times\n^(node::AbstractNode, n::Int) = Repeat(node, n)","category":"section"},{"location":"development/#Custom-Step-Executor","page":"Development","title":"Custom Step Executor","text":"Extend execute for new work types:\n\nstruct HTTPGet\n    url::String\nend\n\nfunction SimplePipelines.execute(step::Step{HTTPGet})\n    start = time()\n    resp = HTTP.get(step.work.url)\n    return StepResult(step, resp.status == 200, time() - start, String(resp.body))\nend\n\n# Usage: Step(:fetch, HTTPGet(\"https://api.example.com\"))","category":"section"},{"location":"development/#Type-Stability-Rules","page":"Development","title":"Type Stability Rules","text":"Use parametric types: struct MyNode{T<:AbstractNode} not children::Vector{AbstractNode}\nUse tuple recursion for heterogeneous collections\nDispatch on types, don't check with isa","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"This page shows pipeline patterns in order of increasing complexity. Each example has a flow diagram, a short goal, and runnable code.\n\nContents\n\nBasics — sequential, parallel, Julia + shell\nControl flow — retry, fallback, branching\nComplex DAGs — multi-stage parallel, robust pipeline\nBioinformatics — immune repertoire (single & multi-donor), variant calling\n\n","category":"section"},{"location":"examples/#1.-Basics","page":"Examples","title":"1. Basics","text":"","category":"section"},{"location":"examples/#1.1-Basic-Pipeline","page":"Examples","title":"1.1 Basic Pipeline","text":"Flow: Three steps in sequence.\n\n  download  ──►  process  ──►  upload\n\nGoal: Create a file, process it, then copy the result (runnable without network).\n\nusing SimplePipelines\n\ndownload = @step download = sh\"(echo line3; echo line1; echo line2) > data.txt\"\nprocess = @step process = sh\"sort data.txt > sorted.txt\"\nupload = @step upload = sh\"cp sorted.txt uploaded.txt\"\n\npipeline = download >> process >> upload\nexecute(pipeline)\n\n","category":"section"},{"location":"examples/#1.2-Parallel-Processing","page":"Examples","title":"1.2 Parallel Processing","text":"Flow: Three steps run in parallel, then one step merges.\n\n       ┌── file_a ──┐\n       ├── file_b ──┼──►  archive\n       └── file_c ──┘\n\nGoal: Create three files, compress them concurrently, then archive (runnable as-is).\n\nusing SimplePipelines\n\nfile_a = @step a = sh\"echo content_a > file_a.txt && gzip -k file_a.txt\"\nfile_b = @step b = sh\"echo content_b > file_b.txt && gzip -k file_b.txt\"\nfile_c = @step c = sh\"echo content_c > file_c.txt && gzip -k file_c.txt\"\narchive = @step archive = sh\"tar -cvf archive.tar file_a.txt.gz file_b.txt.gz file_c.txt.gz\"\n\npipeline = (file_a & file_b & file_c) >> archive\nexecute(pipeline)\n\n","category":"section"},{"location":"examples/#1.3-Julia-Shell","page":"Examples","title":"1.3 Julia + Shell","text":"Flow: Julia step → shell step → Julia step.\n\n  generate  ──►  process  ──►  report\n   (Julia)       (shell)      (Julia)\n\nGoal: Generate data in Julia, run a shell tool, then summarize in Julia.\n\nusing SimplePipelines\nusing DelimitedFiles\n\ngenerate = @step generate = () -> begin\n    data = rand(100, 10)\n    writedlm(\"matrix.csv\", data, ',')\n    return \"Generated $(size(data)) matrix\"\nend\n\nprocess = @step process = sh\"wc -l matrix.csv\"\n\nreport = @step report = () -> begin\n    lines = read(\"matrix.csv\", String)\n    nrows = count(==('\\n'), lines)\n    return \"Matrix has $nrows rows\"\nend\n\npipeline = generate >> process >> report\nexecute(pipeline)\n\n","category":"section"},{"location":"examples/#2.-Control-flow","page":"Examples","title":"2. Control flow","text":"","category":"section"},{"location":"examples/#2.1-Retry-and-Fallback","page":"Examples","title":"2.1 Retry and Fallback","text":"Flow (retry then continue): Run a step up to N times, then continue.\n\n  [fetch^3]  ──►  process\n   (retry)\n\nFlow (fallback): If primary fails, run fallback.\n\n   primary  ──►  (on success)  result\n      │\n      └──►  (on failure)  fallback  ──►  result\n\nGoal: Retry flaky fetch; use fallback when primary step fails; combine retry and fallback.\n\nusing SimplePipelines\n\n# Retry then continue (fetch creates file; no network)\nfetch = @step fetch = sh\"echo '{\\\"x\\\":1}' > data.json\"\nprocess = @step process = sh\"wc -c data.json\"\npipeline = Retry(fetch, 3, delay=0.1) >> process\n\n# Fallback (create data.csv first so both branches have input)\nexecute(@step setup = sh\"(echo 'a,b'; echo '1,2') > data.csv\", verbose=false)\nfast = @step fast = sh\"sort data.csv > sorted.csv\"\nslow = @step slow = sh\"cat data.csv > sorted.csv\"\npipeline = fast | slow\n\n# Retry then fallback\npipeline = Retry(fast, 3) | slow\nexecute(pipeline)\n\n","category":"section"},{"location":"examples/#2.2-Conditional-Branching","page":"Examples","title":"2.2 Conditional Branching","text":"Flow: One of two branches runs based on a condition.\n\n            ┌── if true  ──►  branch_a\n  condition ─┤\n            └── if false ──►  branch_b\n\nGoal: Choose processing path by file size or environment (e.g. DEBUG).\n\nusing SimplePipelines\n\n# Create data.csv so the branch condition has a file to check\nexecute(@step setup = sh\"(echo 'a,b'; echo '1,2'; echo '3,4') > data.csv\", verbose=false)\n\n# By file size\nsmall_pipeline = @step small = sh\"head -n 1000 data.csv > sample.csv\"\nlarge_pipeline = @step decompress = sh\"gunzip -c data.csv.gz > data.csv\" >> @step process = sh\"split -l 10000 data.csv chunk_\"\npipeline = Branch(\n    () -> filesize(\"data.csv\") < 100_000_000,\n    small_pipeline,\n    large_pipeline\n)\nexecute(pipeline)\n\n# By environment\ndebug_steps = @step debug = sh\"echo 'debug mode'\"\nprod_steps = @step prod = sh\"echo 'production'\"\npipeline = Branch(() -> get(ENV, \"DEBUG\", \"0\") == \"1\", debug_steps, prod_steps)\nexecute(pipeline)\n\n","category":"section"},{"location":"examples/#3.-Complex-DAGs","page":"Examples","title":"3. Complex DAGs","text":"","category":"section"},{"location":"examples/#3.1-Multi-stage-Parallel","page":"Examples","title":"3.1 Multi-stage Parallel","text":"Flow: Two parallel fetch+transform branches, then merge, analyze, then report and archive in parallel.\n\n  ┌── fetch_db   ──►  transform_db  ──┐\n  │                                    ├──►  merge  ──►  analyze  ──►  ┌── report\n  └── fetch_files ──►  transform_files ─┘                               └── archive\n\nGoal: Fetch from two sources in parallel (here: create two files), process each, merge, analyze, then report and archive (runnable without network).\n\nusing SimplePipelines\n\nfetch_db = @step db = sh\"echo '{\\\"db\\\":1}' > db_data.json\"\nfetch_files = @step files = sh\"echo 'local_data' > local_data.txt\"\n\ntransform_db = @step transform_db = sh\"wc -c db_data.json > db_size.txt\"\ntransform_files = @step transform_files = sh\"wc -c local_data.txt > files_size.txt\"\n\nmerge = @step merge = sh\"cat db_size.txt files_size.txt > merged.txt\"\nanalyze = @step analyze = sh\"wc -l merged.txt > results.txt\"\n\nreport = @step report = sh\"cat results.txt\"\narchive = @step archive = sh\"gzip -c merged.txt > results.tar.gz\"\n\ndb_branch = fetch_db >> transform_db\nfiles_branch = fetch_files >> transform_files\npipeline = (db_branch & files_branch) >> merge >> analyze >> (report & archive)\n\nexecute(pipeline)\n\n","category":"section"},{"location":"examples/#3.2-Robust-Pipeline-(all-features)","page":"Examples","title":"3.2 Robust Pipeline (all features)","text":"Flow: Retry+fallback fetch → conditional process (small vs full) → report and notify in parallel (notify with retry).\n\n  [primary^3 | backup]  ──►  [small? quick : full]  ──►  report\n                                                      └── notify^2\n\nGoal: Fetch with retry and fallback; branch on file size; run report and notify in parallel (runnable without network).\n\nusing SimplePipelines\n\nprimary_source = @step primary = sh\"echo '{\\\"status\\\":\\\"ok\\\"}' > data.json\"\nbackup_source = @step backup = sh\"echo '{\\\"status\\\":\\\"fallback\\\"}' > data.json\"\nfetch = Retry(primary_source, 3, delay=0.1) | backup_source\n\nquick_process = @step quick = sh\"wc -c data.json > output.txt\"\nfull_process = @step parse = sh\"wc -l data.json > output.txt\" >> @step validate = sh\"wc -c output.txt >> output.txt\"\n\nprocess = Branch(\n    () -> filesize(\"data.json\") < 1_000_000,\n    quick_process,\n    full_process\n)\n\nreport = @step report = sh\"cat output.txt\"\nnotify = @step notify = sh\"echo 'Pipeline done'\"\n\npipeline = fetch >> process >> (report & Retry(notify, 2))\nexecute(Pipeline(pipeline, name=\"Robust ETL\"))\n\n","category":"section"},{"location":"examples/#4.-Bioinformatics","page":"Examples","title":"4. Bioinformatics","text":"Multiple donors / samples: Use ForEach with a {wildcard} pattern to automatically discover files and create parallel branches. The wildcard values propagate via normal Julia $() interpolation.\n\nNote: Examples below use real tool names (PEAR, IgBLAST, BWA, etc.). They are runnable only if those tools are installed and input files exist; otherwise use echo placeholders as in the basic examples.","category":"section"},{"location":"examples/#4.1-Immune-Repertoire-(single-donor)","page":"Examples","title":"4.1 Immune Repertoire (single donor)","text":"Flow: Paired-end FASTQ → merge (PEAR) → FASTQ→FASTA → IgBLAST (V/D/J) → Julia filter by identity.\n\n  R1.fastq ──┐\n             ├──►  pear  ──►  to_fasta  ──►  igblast  ──►  filter_identity  ──►  igblast_filtered.tsv\n  R2.fastq ──┘\n\nGoal: Merge paired-end reads, run IgBLAST with V/D/J references, filter rows by v_identity and j_identity > 90% in Julia.\n\nusing SimplePipelines\nusing DelimitedFiles\n\npear = @step pear = sh\"pear -f R1.fastq -r R2.fastq -o merged\"\nto_fasta = @step to_fasta = sh\"seqtk seq -A merged.assembled.fastq > merged.assembled.fasta\"\nigblast = @step igblast = sh\"igblastn -query merged.assembled.fasta -germline_db_V V.fasta -germline_db_D D.fasta -germline_db_J J.fasta -outfmt 7 -out igblast.tsv\"\n\nfilter_identity = @step filter_identity = () -> begin\n    data, header_row = readdlm(\"igblast.tsv\", '\\t', header=true)\n    header = vec(header_row)\n    v_idx = findfirst(isequal(\"v_identity\"), header)\n    j_idx = findfirst(isequal(\"j_identity\"), header)\n    to_float(x) = something(tryparse(Float64, string(x)), 0.0)\n    filtered = [r for r in eachrow(data) if to_float(r[v_idx]) > 90.0 && to_float(r[j_idx]) > 90.0]\n    writedlm(\"igblast_filtered.tsv\", vcat(header', filtered), '\\t')\n    return \"Filtered $(length(filtered)) sequences\"\nend\n\npipeline = pear >> to_fasta >> igblast >> filter_identity\nexecute(Pipeline(pipeline, name=\"Immune Repertoire\"))\n\n","category":"section"},{"location":"examples/#4.2-Immune-Repertoire-(multiple-donors)","page":"Examples","title":"4.2 Immune Repertoire (multiple donors)","text":"Flow: ForEach discovers files matching pattern → one parallel branch per donor.\n\n  ForEach(\"fastq/{donor}_R1.fq.gz\")\n       │\n       ├── donor1: pear → fasta → igblast  ──┐\n       ├── donor2: pear → fasta → igblast  ──┼── (parallel)\n       └── donorN: pear → fasta → igblast  ──┘\n\nGoal: Run the same pipeline for every donor; files discovered automatically.\n\nusing SimplePipelines\n\n# ForEach: pattern discovery + parallel branches in one construct\npipeline = ForEach(\"fastq/{donor}_R1.fq.gz\") do donor\n    sh(\"pear -f fastq/$(donor)_R1.fq.gz -r fastq/$(donor)_R2.fq.gz -o $(donor)_merged\") >>\n    sh(\"seqtk seq -A $(donor)_merged.assembled.fastq > $(donor).fasta\") >>\n    sh(\"igblastn -query $(donor).fasta -germline_db_V V.fasta -germline_db_D D.fasta -germline_db_J J.fasta -outfmt 7 -out $(donor)_igblast.tsv\")\nend\n\nexecute(pipeline)\n\n","category":"section"},{"location":"examples/#4.3-Variant-Calling","page":"Examples","title":"4.3 Variant Calling","text":"Flow: Paired-end reads → FastQC → trim → BWA (GRCh38) → sort/index → bcftools call → index → filter.\n\n  R1.fq.gz ──┐\n             ├──►  fastqc  ──►  trim  ──►  align  ──►  index  ──►  call  ──►  index_vcf  ──►  filter_vcf\n  R2.fq.gz ──┘\n\nGoal: QC, trim, align to GRCh38, call variants with bcftools, filter by quality.\n\nusing SimplePipelines\n\nfastqc = @step fastqc = sh\"fastqc -o qc/ R1.fq.gz R2.fq.gz\"\ntrim = @step trim = sh\"trimmomatic PE R1.fq.gz R2.fq.gz R1_trimmed.fq.gz R1_unpaired.fq.gz R2_trimmed.fq.gz R2_unpaired.fq.gz ILLUMINACLIP:adapters.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\"\n\nalign = @step align = sh\"bwa mem -t 8 GRCh38.fa R1_trimmed.fq.gz R2_trimmed.fq.gz | samtools sort -@ 4 -o aligned.bam -\"\nindex = @step index = sh\"samtools index aligned.bam\"\n\ncall = @step call = sh\"bcftools mpileup -f GRCh38.fa aligned.bam | bcftools call -mv -Oz -o variants.vcf.gz\"\nindex_vcf = @step index_vcf = sh\"bcftools index variants.vcf.gz\"\nfilter_vcf = @step filter_vcf = sh\"bcftools filter -i 'QUAL>=20' variants.vcf.gz -Oz -o filtered.vcf.gz\"\n\npipeline = fastqc >> trim >> align >> index >> call >> index_vcf >> filter_vcf\nexecute(Pipeline(pipeline, name=\"Variant Calling\"))","category":"section"},{"location":"#SimplePipelines.jl","page":"Home","title":"SimplePipelines.jl","text":"Minimal, type-stable DAG pipelines for Julia","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"SimplePipelines.jl lets you define and execute directed acyclic graph (DAG) pipelines using intuitive operators.","category":"section"},{"location":"#Steps","page":"Home","title":"Steps","text":"Syntax Description\n@step name = sh\"cmd\" Shell command\n@step name = sh\"cmd > file\" Shell with redirection/pipes\n@step name = () -> expr Julia function","category":"section"},{"location":"#Operators","page":"Home","title":"Operators","text":"Operator Description Example\n>> Sequential a >> b >> c\n& Parallel a & b & c\n| Fallback a | b (b if a fails)\n^n Retry a^3 (up to 3 times)","category":"section"},{"location":"#Control-Flow","page":"Home","title":"Control Flow","text":"Function Description\nTimeout(a, secs) Fail if exceeds time\nBranch(cond, a, b) Conditional execution\nMap(f, items) Fan-out over collection\nReduce(f, a & b) Combine parallel outputs\nForEach(pattern) do ... Discover files by pattern\nRetry(a, n, delay=d) Retry with delay","category":"section"},{"location":"#Execution-and-Results","page":"Home","title":"Execution & Results","text":"Function / Field Description\nexecute(p) Run, return results\nexecute(p, verbose=false) Run silently\nexecute(p, dry_run=true) Preview only\nresults[i].success Did step succeed?\nresults[i].duration Time in seconds\nresults[i].output Output or error","category":"section"},{"location":"#Utilities","page":"Home","title":"Utilities","text":"Function Description\nprint_dag(node) Visualize structure\ncount_steps(node) Count steps\nsteps(node) Get all steps","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"using SimplePipelines\n\n# Simple sequence\npipeline = sh\"echo 'step 1'\" >> sh\"echo 'step 2'\" >> sh\"echo 'step 3'\"\nexecute(pipeline)\n\n# Parallel branches that merge\npipeline = (sh\"process A\" & sh\"process B\") >> sh\"merge\"\nexecute(pipeline)","category":"section"},{"location":"#DAG-Patterns","page":"Home","title":"DAG Patterns","text":"","category":"section"},{"location":"#Diamond-(fork-join)","page":"Home","title":"Diamond (fork-join)","text":"       ┌── step_b ──┐\nstep_a─┤            ├── step_d\n       └── step_c ──┘\n\nstep_a = @step a = sh\"fetch\"\nstep_b = @step b = sh\"analyze_a\"\nstep_c = @step c = sh\"analyze_b\"\nstep_d = @step d = sh\"report\"\npipeline = step_a >> (step_b & step_c) >> step_d","category":"section"},{"location":"#Multi-stage-parallel","page":"Home","title":"Multi-stage parallel","text":"       ┌─ b ─┐     ┌─ e ─┐\n    a ─┤     ├─ d ─┤     ├─ g\n       └─ c ─┘     └─ f ─┘\n\na = @step a = sh\"stage_a\"\nb = @step b = sh\"stage_b\"\nc = @step c = sh\"stage_c\"\nd = @step d = sh\"stage_d\"\ne = @step e = sh\"stage_e\"\nf = @step f = sh\"stage_f\"\ng = @step g = sh\"stage_g\"\npipeline = a >> (b & c) >> d >> (e & f) >> g","category":"section"},{"location":"#Independent-branches-merging","page":"Home","title":"Independent branches merging","text":"    ┌─ a ── b ─┐\n    │          │\n    ├─ c ── d ─┼── merge\n    │          │\n    └─ e ── f ─┘\n\na = @step a = sh\"branch1_a\"\nb = @step b = sh\"branch1_b\"\nc = @step c = sh\"branch2_a\"\nd = @step d = sh\"branch2_b\"\ne = @step e = sh\"branch3_a\"\nf = @step f = sh\"branch3_b\"\nmerge = @step merge = sh\"merge\"\nbranch1 = a >> b\nbranch2 = c >> d\nbranch3 = e >> f\npipeline = (branch1 & branch2 & branch3) >> merge","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Intuitive operators - >> sequence, & parallel, | fallback, ^ retry\nControl flow - Timeout, Branch, Map for complex workflows\nType-stable - Zero runtime type checks, full compile-time specialization\nMinimal overhead - @inline functions and tuple recursion\nComposable - All operators work together seamlessly\nUnified interface - Shell commands and Julia functions compose seamlessly","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"Pages = [\"tutorial.md\", \"examples.md\", \"api.md\", \"design.md\", \"development.md\"]\nDepth = 2","category":"section"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#Steps","page":"Tutorial","title":"Steps","text":"A Step is the basic unit of work—either a shell command or Julia function.","category":"section"},{"location":"tutorial/#Shell-Commands","page":"Tutorial","title":"Shell Commands","text":"using SimplePipelines\n\n# Direct command (anonymous step)\nstep = @step sh\"samtools sort input.bam\"\n\n# Named step\nstep = @step sort = sh\"samtools sort input.bam\"\n\n# Shell features (>, |, &&) - use sh\"...\"\nstep = @step sort = sh\"sort data.txt | uniq > sorted.txt\"","category":"section"},{"location":"tutorial/#Julia-Functions","page":"Tutorial","title":"Julia Functions","text":"# Anonymous function step\nstep = @step () -> process_data()\n\n# Named function step\nstep = @step analyze = () -> run_analysis(\"data.csv\")","category":"section"},{"location":"tutorial/#File-Dependencies-(Optional)","page":"Tutorial","title":"File Dependencies (Optional)","text":"Track input/output files for validation:\n\n@step align(\"reads.fq\" => \"aligned.bam\") = sh\"bwa mem ref.fa reads.fq > aligned.bam\"  # sh\"...\" for redirection","category":"section"},{"location":"tutorial/#Sequential-Execution:","page":"Tutorial","title":"Sequential Execution: >>","text":"The >> operator chains steps—each waits for the previous to complete:\n\n# Chain commands directly (anonymous steps)\npipeline = sh\"download data.txt\" >> sh\"process data.txt\" >> sh\"upload results.txt\"\n\n# Or define named steps, then chain\nstep_a = @step step_a = sh\"download data.txt\"\nstep_b = @step step_b = sh\"process data.txt\"\nstep_c = @step step_c = sh\"upload results.txt\"\npipeline = step_a >> step_b >> step_c","category":"section"},{"location":"tutorial/#Parallel-Execution:-and","page":"Tutorial","title":"Parallel Execution: &","text":"The & operator groups steps to run concurrently:\n\n# Parallel steps (anonymous)\nparallel = sh\"task_a\" & sh\"task_b\" & sh\"task_c\"\n\n# Named steps in parallel, then merge\nsample_1 = @step s1 = sh\"process sample1\"\nsample_2 = @step s2 = sh\"process sample2\"\nsample_3 = @step s3 = sh\"process sample3\"\nmerge_results = @step merge = sh\"merge outputs\"\npipeline = (sample_1 & sample_2 & sample_3) >> merge_results","category":"section"},{"location":"tutorial/#Complex-DAGs","page":"Tutorial","title":"Complex DAGs","text":"Combine >> and & for arbitrary graphs.","category":"section"},{"location":"tutorial/#Diamond-Pattern","page":"Tutorial","title":"Diamond Pattern","text":"       ┌── analyze_a ──┐\n fetch─┤               ├── report\n       └── analyze_b ──┘\n\nfetch = @step fetch = sh\"echo 'a,b\\n1,2' > data.csv\"\nanalyze_a = @step a = sh\"wc -l data.csv\"\nanalyze_b = @step b = sh\"wc -c data.csv\"\nreport = @step report = () -> \"done\"\n\npipeline = fetch >> (analyze_a & analyze_b) >> report\nexecute(pipeline)","category":"section"},{"location":"tutorial/#Multi-Stage-Parallel","page":"Tutorial","title":"Multi-Stage Parallel","text":"For graphs with multiple fork-join points, compose in stages:\n\n     ┌─ b ─┐     ┌─ e ─┐\n  a ─┤     ├─ d ─┤     ├─ g\n     └─ c ─┘     └─ f ─┘\n\na = @step a = sh\"step_a\"\nb = @step b = sh\"step_b\"\nc = @step c = sh\"step_c\"\nd = @step d = sh\"step_d\"\ne = @step e = sh\"step_e\"\nf = @step f = sh\"step_f\"\ng = @step g = sh\"step_g\"\n\npipeline = a >> (b & c) >> d >> (e & f) >> g","category":"section"},{"location":"tutorial/#Independent-Branches","page":"Tutorial","title":"Independent Branches","text":"Process independent pipelines in parallel, then merge:\n\n  ┌─ fetch_a >> process_a ─┐\n  │                        │\n  ├─ fetch_b >> process_b ─┼── merge\n  │                        │\n  └─ fetch_c >> process_c ─┘\n\nfetch_a = @step fetch_a = sh\"fetch sample_a\"\nprocess_a = @step process_a = sh\"process sample_a\"\nfetch_b = @step fetch_b = sh\"fetch sample_b\"\nprocess_b = @step process_b = sh\"process sample_b\"\nfetch_c = @step fetch_c = sh\"fetch sample_c\"\nprocess_c = @step process_c = sh\"process sample_c\"\nmerge = @step merge = sh\"merge results\"\n\nbranch_a = fetch_a >> process_a\nbranch_b = fetch_b >> process_b\nbranch_c = fetch_c >> process_c\npipeline = (branch_a & branch_b & branch_c) >> merge\n\nThis pattern is common for processing multiple samples/files independently before combining results.","category":"section"},{"location":"tutorial/#Fallback:","page":"Tutorial","title":"Fallback: |","text":"The | operator provides fallback behavior—if the primary fails, run the fallback:\n\n# If fast method fails, use slow method\nfast_method = @step fast = sh\"fast_tool input.txt\"\nslow_method = @step slow = sh\"slow_tool input.txt\"\npipeline = fast_method | slow_method\n\n# Chain multiple fallbacks\nmethod_a = @step a = sh\"method_a input\"\nmethod_b = @step b = sh\"method_b input\"\nmethod_c = @step c = sh\"method_c input\"\npipeline = method_a | method_b | method_c","category":"section"},{"location":"tutorial/#Retry:-or-Retry()","page":"Tutorial","title":"Retry: ^ or Retry()","text":"Retry a node up to N times on failure:\n\n# Using ^ operator (concise) – retry flaky step up to 3 times\nflaky_api_call = @step api = sh\"echo 'mock response'\"\npipeline = flaky_api_call^3\n\n# Using Retry() with delay between attempts\nnetwork_request = @step fetch = sh\"echo 'data'\"\npipeline = Retry(network_request, 5, delay=2.0)\n\n# Combine with fallback\nprimary = @step primary = sh\"echo primary\"\nfallback = @step fallback = sh\"echo fallback\"\npipeline = primary^3 | fallback","category":"section"},{"location":"tutorial/#Branch-(Conditional)","page":"Tutorial","title":"Branch (Conditional)","text":"Execute different branches based on a runtime condition:\n\n# Branch based on file size\nlarge_file_pipeline = @step large = sh\"process_large data.txt\"\nsmall_file_pipeline = @step small = sh\"process_small data.txt\"\npipeline = Branch(\n    () -> filesize(\"data.txt\") > 1_000_000,\n    large_file_pipeline,\n    small_file_pipeline\n)\n\n# Branch based on environment\ndebug_steps = @step debug = sh\"run with verbose logging\"\nnormal_steps = @step normal = sh\"run quietly\"\npipeline = Branch(\n    () -> haskey(ENV, \"DEBUG\"),\n    debug_steps,\n    normal_steps\n)","category":"section"},{"location":"tutorial/#Timeout","page":"Tutorial","title":"Timeout","text":"Fail if a node exceeds a time limit:\n\n# 30 second timeout\npipeline = Timeout(long_running_step, 30.0)\n\n# Combine with retry and fallback\npipeline = Timeout(api_call, 5.0)^3 | backup","category":"section"},{"location":"tutorial/#Map-(Fan-out)","page":"Tutorial","title":"Map (Fan-out)","text":"Apply a function to each item, creating parallel steps:\n\n# Process files in parallel (list supplied in code)\nsamples = [\"sample_A\", \"sample_B\", \"sample_C\"]\npipeline = Map(samples) do s\n    Step(Symbol(\"process_\", s), sh(\"analyze $s.fastq\"))\nend >> merge_results","category":"section"},{"location":"tutorial/#ForEach-(Pattern-based-discovery)","page":"Tutorial","title":"ForEach (Pattern-based discovery)","text":"Discover files by pattern, create parallel branches automatically:\n\n# Single step per file - use sh(\"...\") for interpolation\nForEach(\"{sample}.fastq\") do sample\n    sh(\"process $(sample).fastq\")\nend\n\n# Multi-step per file - chain with >>\nForEach(\"fastq/{sample}_R1.fq.gz\") do sample\n    sh(\"pear $(sample)_R1 $(sample)_R2\") >> sh(\"analyze $(sample)\")\nend\n\n# Multiple wildcards\nForEach(\"data/{project}/{sample}.csv\") do project, sample\n    sh(\"process $(project)/$(sample).csv\")\nend\n\n# Chain with downstream merge\nForEach(\"{id}.fastq\") do id\n    sh(\"align $(id).fastq\")\nend >> @step merge = sh\"merge *.bam\"","category":"section"},{"location":"tutorial/#Reduce-(Combine)","page":"Tutorial","title":"Reduce (Combine)","text":"Collect outputs from parallel steps and combine them:\n\n# Combine parallel outputs with a function\npipeline = Reduce(analyze_a & analyze_b) do outputs\n    join(outputs, \"\\n\")\nend\n\n# Using a named function (define reducer and steps first)\ncombine_results(outputs) = join(outputs, \"\\n\")\nstep_a = @step a = sh\"echo result_a\"\nstep_b = @step b = sh\"echo result_b\"\nstep_c = @step c = sh\"echo result_c\"\npipeline = Reduce(combine_results, step_a & step_b & step_c)\n\n# In a pipeline: fetch -> parallel analysis -> reduce -> report\npipeline = fetch >> Reduce(merge, analyze_a & analyze_b) >> report\n\nThe reducer function receives a Vector{String} of outputs from all successful parallel steps.","category":"section"},{"location":"tutorial/#Running-Pipelines","page":"Tutorial","title":"Running Pipelines","text":"Use execute(pipeline) to run:\n\n# Basic execution\nresults = execute(pipeline)\n\n# Silent (no progress output)\nresults = execute(pipeline, verbose=false)\n\n# Dry run (preview structure)\nexecute(pipeline, dry_run=true)\n\n# Named pipeline\nstep_a = @step a = sh\"first\"\nstep_b = @step b = sh\"second\"\np = Pipeline(step_a >> step_b, name=\"My Workflow\")\nexecute(p)","category":"section"},{"location":"tutorial/#Checking-Results","page":"Tutorial","title":"Checking Results","text":"results = execute(pipeline)\n\nfor r in results\n    if r.success\n        println(\"$(r.step.name): completed in $(r.duration)s\")\n    else\n        println(\"$(r.step.name): FAILED - $(r.output)\")\n    end\nend\n\n# Check overall success\nall_ok = all(r -> r.success, results)","category":"section"},{"location":"tutorial/#Mixing-Shell-and-Julia","page":"Tutorial","title":"Mixing Shell and Julia","text":"Shell commands and Julia functions compose seamlessly:\n\n# Julia: prepare data (e.g. filter non-empty lines)\nprep = @step prep = () -> begin\n    raw = read(\"raw.csv\", String)\n    cleaned = filter(line -> !isempty(strip(line)), split(raw, '\\n'))\n    write(\"clean.csv\", join(cleaned, '\\n'))\n    return \"Wrote $(length(cleaned)) lines\"\nend\n\n# Shell: run external tool\nexternal = @step tool = sh\"wc -l clean.csv > result.txt\"  # sh\"...\" for redirection\n\n# Julia: postprocess\npost = @step post = () -> begin\n    n = parse(Int, split(read(\"result.txt\", String))[1])\n    return \"Line count: $n\"\nend\n\npipeline = prep >> external >> post\nexecute(pipeline)","category":"section"},{"location":"tutorial/#Utilities","page":"Tutorial","title":"Utilities","text":"# Count steps in a pipeline\nn = count_steps(pipeline)\n\n# Get all steps as a vector\nall_steps = steps(pipeline)\n\n# Print DAG structure\nprint_dag(pipeline)","category":"section"}]
}
