var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Module","page":"API Reference","title":"Module","text":"","category":"section"},{"location":"api/#Types","page":"API Reference","title":"Types","text":"","category":"section"},{"location":"api/#Macros","page":"API Reference","title":"Macros","text":"","category":"section"},{"location":"api/#Operators","page":"API Reference","title":"Operators","text":"The package extends these operators for pipeline composition. Cmd and Function arguments are auto-wrapped in Step.\n\nOperator Name Description\n>> Sequence Run nodes in order\n& Parallel Run nodes concurrently\n| Fallback Run fallback if primary fails\n^ Retry Wrap with retries, e.g. node^3","category":"section"},{"location":"api/#Functions","page":"API Reference","title":"Functions","text":"","category":"section"},{"location":"api/#Shell","page":"API Reference","title":"Shell","text":"","category":"section"},{"location":"api/#Execution","page":"API Reference","title":"Execution","text":"Execution is recursive: run(pipeline) calls run_node(root, ...) which dispatches on node type and recurses (Sequence in order, Parallel/ForEach/Map with optional @spawn).","category":"section"},{"location":"api/#Freshness-and-state","page":"API Reference","title":"Freshness and state","text":"State is stored in .pipeline_state as a fixed-layout, memory-mapped file. Completions are batched and written when run() finishes.","category":"section"},{"location":"api/#State-file-format","page":"API Reference","title":"State file format","text":"The state file uses a fixed binary layout (see src/StateFormat.jl) for random access and mmap.","category":"section"},{"location":"api/#Utilities","page":"API Reference","title":"Utilities","text":"","category":"section"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/#SimplePipelines","page":"API Reference","title":"SimplePipelines","text":"SimplePipelines\n\nMinimal, type-stable DAG pipelines for Julia with Make-like incremental builds. Execution is recursive: run_node dispatches on node type and recurses (Sequence in order, Parallel with @spawn, ForEach/Map expand then run).\n\nQuick Start\n\nusing SimplePipelines\n\n# Chain steps with >>\npipeline = sh\"echo hello\" >> sh\"echo world\"\n\n# Run in parallel with &\npipeline = sh\"task_a\" & sh\"task_b\"\n\n# Fallback on failure with |\npipeline = sh\"primary\" | sh\"backup\"\n\n# Named steps with file dependencies\ndownload = @step download([] => [\"data.csv\"]) = sh\"curl -o data.csv http://example.com\"\nprocess = @step process([\"data.csv\"] => [\"out.csv\"]) = sh\"sort data.csv > out.csv\"\n\nrun(download >> process)\n\nFeatures\n\nRecursive execution: Dispatch on node type; Sequence runs in order, Parallel/ForEach/Map run branches with optional parallelism.\nMake-like freshness: Steps skip if outputs are newer than inputs.\nState persistence: Tracks completed steps across runs.\nColored output: Visual tree structure with status indicators.\nForce execution: Force(step) or run(p, force=true).\n\nSee also: Step, Pipeline, run, is_fresh.\n\n\n\n\n\n","category":"module"},{"location":"api/#SimplePipelines.AbstractNode","page":"API Reference","title":"SimplePipelines.AbstractNode","text":"AbstractNode\n\nAbstract supertype of all pipeline nodes (Step, Sequence, Parallel, Retry, Fallback, Branch, Timeout, Force, Reduce, Map, ForEach). Constructors only build the struct; execution is via the functor: call (node)(v, forced) which dispatches to run_node(node, v, forced).\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.AbstractStepResult","page":"API Reference","title":"SimplePipelines.AbstractStepResult","text":"Supertype of all step results; use for Vector{AbstractStepResult} (e.g. return of run).\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.StepResult","page":"API Reference","title":"SimplePipelines.StepResult","text":"StepResult(step, success, duration, output [; value])\n\nResult of running one step. Type is StepResult{S, V}: S is the step type, V is the type of value (e.g. Nothing for command steps, DataFrame when the step returns one). Use results[i].value for the actual return value; output is a string for display/logging.\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Step","page":"API Reference","title":"SimplePipelines.Step","text":"Step{F} <: AbstractNode\n\nA single unit of work in a pipeline. F is the work type (Cmd or Function).\n\nFields\n\nname::Symbol — Step identifier (auto-generated if not provided)\nwork::F — The command or function to execute\ninputs::Vector{String} — Input file dependencies\noutputs::Vector{String} — Output file paths\n\nConstructors\n\nStep(work)                           # Auto-generated name\nStep(name::Symbol, work)             # Named step\nStep(name, work, inputs, outputs)    # Full specification\n@step name = work                    # Macro form\n@step name(inputs => outputs) = work # Macro with dependencies\n\nExamples\n\n# Command step\nStep(`sort data.txt`)\nStep(:sort, `sort data.txt`)\n\n# Function step  \nStep(() -> println(\"hello\"))\nStep(:greet, () -> println(\"hello\"))\n\n# With file dependencies (enables Make-like freshness)\nStep(:process, `sort in.txt > out.txt`, [\"in.txt\"], [\"out.txt\"])\n\nSee also: @step, is_fresh, Force\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Sequence","page":"API Reference","title":"SimplePipelines.Sequence","text":"Sequence{T} <: AbstractNode\n\nExecutes nodes sequentially, stopping on first failure. Created automatically by the >> operator.\n\na >> b >> c  # equivalent to Sequence((a, b, c))\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Parallel","page":"API Reference","title":"SimplePipelines.Parallel","text":"Parallel{T} <: AbstractNode\n\nExecutes nodes concurrently using threads. Created automatically by the & operator.\n\na & b & c  # equivalent to Parallel((a, b, c))\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Retry","page":"API Reference","title":"SimplePipelines.Retry","text":"Retry{N} <: AbstractNode\n\nRetries a node up to max_attempts times on failure, with optional delay. Created by the ^ operator or Retry() constructor.\n\nstep^3                      # Retry up to 3 times\nRetry(step, 5; delay=2.0)   # 5 attempts with 2s delay between\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Fallback","page":"API Reference","title":"SimplePipelines.Fallback","text":"Fallback{A,B} <: AbstractNode\n\nExecutes fallback node if primary fails. Created by the | operator.\n\nprimary | backup  # Run backup if primary fails\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Branch","page":"API Reference","title":"SimplePipelines.Branch","text":"Branch{C,T,F} <: AbstractNode\n\nConditional execution based on a predicate function.\n\nBranch(() -> should_update(), update_step, skip_step)\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Timeout","page":"API Reference","title":"SimplePipelines.Timeout","text":"Timeout{N} <: AbstractNode\n\nWraps a node with a time limit. Returns failure if time exceeded.\n\nTimeout(slow_step, 30.0)  # 30 second timeout\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Reduce","page":"API Reference","title":"SimplePipelines.Reduce","text":"Reduce{F,N} <: AbstractNode\n\nRuns a parallel node and combines successful step outputs with a reducer function.\n\nReduce((a & b), join)           # Reduce(join, a & b)\nReduce(a & b)(join)             # curried: reducer later\nReduce(join, a & b; name=:merge)\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Force","page":"API Reference","title":"SimplePipelines.Force","text":"Force{N} <: AbstractNode\n\nForces execution of a node, bypassing freshness checks. Use when you need to re-run a step even if outputs are up-to-date.\n\nForce(step)              # Force this specific step\nrun(pipeline, force=true) # Force all steps in pipeline\n\nSee also: is_fresh, clear_state!\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Pipeline","page":"API Reference","title":"SimplePipelines.Pipeline","text":"Pipeline{N<:AbstractNode}\n\nA named pipeline wrapping a root node for execution.\n\np = Pipeline(step1 >> step2, name=\"ETL\")\nrun(p)\ndisplay(p)  # Shows tree structure\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.@step","page":"API Reference","title":"SimplePipelines.@step","text":"@step name = work\n@step name(inputs => outputs) = work\n@step work\n\nCreate a named step with optional file dependencies. Steps are lazy: if the right-hand side is a function call (e.g. println(x) or run_cmd(path)), it is wrapped in a thunk and runs only when the pipeline is run via run(pipeline). Building the pipeline or inspecting it (e.g. print_dag) does not execute step work. Shell commands (backtick/sh\"...\") are also only executed when the step runs.\n\nExamples\n\n# Named step\n@step download = sh\"curl -o data.csv http://example.com\"\n\n# With file dependencies (enables Make-like freshness checks)\n@step process([\"input.csv\"] => [\"output.csv\"]) = sh\"sort input.csv > output.csv\"\n\n# Anonymous step (auto-named)\n@step sh\"echo hello\"\n\n\n\n\n\n","category":"macro"},{"location":"api/#SimplePipelines.@sh_str","page":"API Reference","title":"SimplePipelines.@sh_str","text":"sh\"command\"\nsh(command::String)\n\nShell commands: use sh\"...\" for literals, sh(\"...\") when you need interpolation.\n\nExamples\n\nsh\"sort data.txt > sorted.txt\"\nsh(\"process \\$(sample)_R1.fq\")\n\n\n\n\n\n","category":"macro"},{"location":"api/#SimplePipelines.Map","page":"API Reference","title":"SimplePipelines.Map","text":"Lazy node: applies f to each item when the pipeline runs (not at construction).\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.ForEach","page":"API Reference","title":"SimplePipelines.ForEach","text":"Lazy node: discovers files and runs the block once per match when the pipeline runs (not at construction).\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.fe","page":"API Reference","title":"SimplePipelines.fe","text":"Short alias for ForEach. Use fe(\"pattern\") do x ... end.\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.sh","page":"API Reference","title":"SimplePipelines.sh","text":"Shell command with string interpolation. See also @sh_str.\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.run","page":"API Reference","title":"Base.run","text":"run(p::Pipeline; verbose=true, dry_run=false, force=false, jobs=8) -> Vector{AbstractStepResult}\nrun(node::AbstractNode; kwargs...) -> Vector{AbstractStepResult}\n\nExecute a pipeline or node, returning results for each step.\n\nKeywords\n\nverbose=true: Show colored progress output\ndry_run=false: If true, show DAG structure without executing\nforce=false: If true, run all steps regardless of freshness\njobs=8: Max concurrent branches for Parallel/ForEach/Map. All branches run; when jobs > 0, they run in rounds of jobs (each round waits for the previous). jobs=0 = unbounded (all at once).\n\nOutput\n\nWith verbose=true, shows tree-structured output: ▶ running, ✓ success, ✗ failure, ⊳ up to date (not re-run).\n\nExamples\n\nrun(pipeline)                # Default: up to 8 branches at a time\nrun(pipeline, jobs=0)         # Unbounded (all branches at once)\nrun(pipeline, jobs=4)         # At most 4 concurrent\nrun(pipeline, verbose=false)\nrun(pipeline, dry_run=true)\nrun(pipeline, force=true)\n\nSee also: is_fresh, Force, print_dag\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.is_fresh","page":"API Reference","title":"SimplePipelines.is_fresh","text":"is_fresh(step::Step) -> Bool\n\nCheck if a step can be skipped based on Make-like freshness rules.\n\nFreshness Rules\n\nHas inputs and outputs: Fresh if all outputs exist and are newer than all inputs\nHas only outputs: Fresh if outputs exist and step was previously completed\nNo file dependencies: Fresh if step was previously completed (state-based tracking)\n\nState is persisted in .pipeline_state as a fixed-layout, memory-mapped file (see StateFormat in src/StateFormat.jl: magic, count, then hash array). Completions during a run are batched and written once when run() finishes.\n\nExamples\n\nstep = @step process([\"in.txt\"] => [\"out.txt\"]) = sh\"sort in.txt > out.txt\"\nis_fresh(step)  # true if out.txt exists and is newer than in.txt\n\nSee also: Force, clear_state!\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.clear_state!","page":"API Reference","title":"SimplePipelines.clear_state!","text":"clear_state!()\n\nRemove the pipeline state file (.pipeline_state), forcing all steps to run on the next execution regardless of freshness. The file uses the fixed binary layout defined in the StateFormat module.\n\nSee also: is_fresh, Force\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.StateFormat","page":"API Reference","title":"SimplePipelines.StateFormat","text":"StateFormat\n\nFixed binary layout for the pipeline state file. Single source of truth for offsets, sizes, and all I/O: memory-mapped file with random access to the hash array.\n\nLayout (on disk):   [0..8)   magic (8 bytes)   [8..16)  count (UInt64, number of valid hashes)   [16..)   hashes (max_hashes × sizeof(UInt64), mmap-able for random access)\n\nAll read/write/mmap of the state file goes through this module (stateinit, stateread, statewrite, stateappend).\n\n\n\n\n\n","category":"module"},{"location":"api/#SimplePipelines.StateFormat.StateFileLayout","page":"API Reference","title":"SimplePipelines.StateFormat.StateFileLayout","text":"StateFileLayout\n\nDescriptor for the fixed binary layout of the state file. Holds magic bytes, header length, offsets for count and hashes, and max capacity.\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.StateFormat.state_init","page":"API Reference","title":"SimplePipelines.StateFormat.state_init","text":"Ensure the state file exists and has the correct size; create or truncate if needed.\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.StateFormat.state_read","page":"API Reference","title":"SimplePipelines.StateFormat.state_read","text":"Read completed step hashes from the state file (memory-mapped, random access). Returns Set{UInt64}.\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.StateFormat.state_write","page":"API Reference","title":"SimplePipelines.StateFormat.state_write","text":"Write all completed hashes to the state file (capped at layout.max_hashes).\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.StateFormat.state_append","page":"API Reference","title":"SimplePipelines.StateFormat.state_append","text":"Append one hash; returns false if at capacity (caller should statewrite(union(stateread(), [h]))).\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.count_steps","page":"API Reference","title":"SimplePipelines.count_steps","text":"count_steps(node) -> Int\n\nReturn the number of steps in the DAG (leaf count for execution).\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.steps","page":"API Reference","title":"SimplePipelines.steps","text":"steps(node) -> Vector{Step}\n\nReturn all leaf steps in the DAG (flattened).\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.print_dag","page":"API Reference","title":"SimplePipelines.print_dag","text":"print_dag(node [; color=true])\nprint_dag(io, node [, indent])\n\nPrint a tree visualization of the pipeline DAG. With color=true (default when writing to a terminal), uses colors for node types and status. See also run and display(pipeline).\n\n\n\n\n\n","category":"function"},{"location":"design/#Design","page":"Design","title":"Design","text":"","category":"section"},{"location":"design/#Interface-Overview","page":"Design","title":"Interface Overview","text":"┌─────────────────────────────────────────────────────────────┐\n│                    SimplePipelines.jl                       │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  @step name = sh\"cmd\"       Shell step                      │\n│  @step name = sh\"cmd > f\"   Shell with redirection/pipes    │\n│  @step name = () -> ...     Julia step                      │\n│                                                             │\n│  a >> b                     Sequential: a then b            │\n│  a & b                      Parallel: a and b together      │\n│                                                             │\n│  run(p)                Execute the pipeline            │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘","category":"section"},{"location":"design/#Type-Hierarchy","page":"Design","title":"Type Hierarchy","text":"AbstractNode\n    │\n    ├── Step{F}           Single unit of work\n    │                     F = Cmd | Function\n    │\n    ├── Sequence{T}       Sequential execution\n    │                     T = Tuple of nodes\n    │\n    └── Parallel{T}       Concurrent execution\n                          T = Tuple of nodes\n\nAll types are fully parametric—the compiler knows exact types at every level.","category":"section"},{"location":"design/#Composition-Model","page":"Design","title":"Composition Model","text":"# User writes:\nsh\"echo a\" >> (sh\"echo b\" & sh\"echo c\") >> sh\"echo d\"\n\n# Becomes:\nSequence{Tuple{\n    Step{Cmd},\n    Parallel{Tuple{Step{Cmd}, Step{Cmd}}},\n    Step{Cmd}\n}}\n\nThe complete structure is encoded in the type, enabling full compile-time specialization.","category":"section"},{"location":"design/#Execution-Flow","page":"Design","title":"Execution Flow","text":"Execution is recursive: dispatch on node type and recurse.\n\nrun(Pipeline)\n       │\n       ▼\nrun_node(root, v, force)  ─── dispatch on node type\n       │\n       ├─► Step:     execute(step) → StepResult\n       ├─► Sequence: run_node each in order; break on first failure\n       ├─► Parallel: @spawn run_node each; fetch and concat\n       ├─► ForEach:  find matches, get nodes from block (cycle check), then run like Parallel\n       ├─► Map:      get nodes from f(item) (cycle check), then run like Parallel\n       └─► Retry/Fallback/Branch/Timeout/Force/Reduce: recurse on inner node(s)\n       │\n       ▼\nVector{AbstractStepResult}","category":"section"},{"location":"design/#Key-Design-Decisions","page":"Design","title":"Key Design Decisions","text":"","category":"section"},{"location":"design/#1.-Tuples,-Not-Vectors","page":"Design","title":"1. Tuples, Not Vectors","text":"# ✗ Vector: type information lost\nSequence(nodes::Vector{AbstractNode})\n\n# ✓ Tuple: exact types preserved\nSequence{Tuple{Step{Cmd}, Step{Function}}}\n\nTuples enable the compiler to generate specialized code for each node.","category":"section"},{"location":"design/#2.-Multiple-Dispatch,-Not-Type-Checks","page":"Design","title":"2. Multiple Dispatch, Not Type Checks","text":"# ✗ Runtime type checking (type unstable)\nfunction run_node(node)\n    if node isa Step\n        # ...\n    elseif node isa Sequence\n        # ...\n    end\nend\n\n# ✓ Multiple dispatch (type stable)\nrun_node(step::Step, v) = execute(step)\nrun_node(seq::Sequence, v) = _run_sequence!([], seq.nodes, v)\nrun_node(par::Parallel, v) = _spawn_parallel(par.nodes, v)","category":"section"},{"location":"design/#3.-Tuple-Recursion","page":"Design","title":"3. Tuple Recursion","text":"Iterate tuples in a type-stable way:\n\n# Base case\n_run_sequence!(results, ::Tuple{}, v) = nothing\n\n# Recursive case\nfunction _run_sequence!(results, nodes::Tuple, v)\n    append!(results, run_node(first(nodes), v))\n    _run_sequence!(results, Base.tail(nodes), v)\nend\n\nThe compiler unrolls this into efficient, specialized code.","category":"section"},{"location":"design/#4.-Verbosity-as-Types","page":"Design","title":"4. Verbosity as Types","text":"struct Verbose end\nstruct Silent end\n\nprint_start(::Silent, ::Step) = nothing\nprint_start(::Verbose, s::Step) = println(\"▶ $(s.name)\")\n\nDead code elimination removes printing when verbose=false.","category":"section"},{"location":"design/#Performance-Characteristics","page":"Design","title":"Performance Characteristics","text":"Aspect Design Choice Benefit\nNode storage Tuples Full type info, inline storage\nDispatch Multiple dispatch Zero runtime type checks\nIteration Recursion Compiler unrolling\nOperators @inline Zero call overhead\nVerbosity Singleton types Dead code elimination","category":"section"},{"location":"development/#Extending-SimplePipelines","page":"Development","title":"Extending SimplePipelines","text":"SimplePipelines is designed for easy extension via Julia's multiple dispatch.","category":"section"},{"location":"development/#Testing-and-coverage","page":"Development","title":"Testing and coverage","text":"Run the test suite:\n\njulia --project=. -e 'using Pkg; Pkg.test()'\n\nRun tests with code coverage (for local profiling or CI):\n\njulia --project=. -e 'using Pkg; Pkg.test(coverage=true)'\n\nCoverage is reported in lcov.info; CI uploads it to Codecov.","category":"section"},{"location":"development/#Architecture","page":"Development","title":"Architecture","text":"All nodes inherit from AbstractNode. To add custom behavior, define a new subtype and implement dispatch methods:\n\nAbstractNode\n    ├── Step{F}           # Leaf node\n    ├── Sequence{T}       # Sequential\n    ├── Parallel{T}       # Concurrent\n    ├── Retry{N}          # Retry on failure\n    ├── Fallback{A,B}     # Try A, else B\n    └── Branch{C,T,F}     # Conditional","category":"section"},{"location":"development/#Custom-Node-in-4-Steps","page":"Development","title":"Custom Node in 4 Steps","text":"using SimplePipelines\nimport SimplePipelines: AbstractNode, run_node, print_dag, count_steps, steps\n\n# 1. Define type (parametric for type stability)\nstruct Timeout{N<:AbstractNode} <: AbstractNode\n    node::N\n    seconds::Float64\nend\n\n# 2. Implement execution\nfunction run_node(t::Timeout, verbosity)\n    # ... timeout logic ...\n    return run_node(t.node, verbosity)\nend\n\n# 3. Implement visualization\nprint_dag(t::Timeout, indent::Int) = begin\n    println(\"  \"^indent, \"Timeout($(t.seconds)s):\")\n    print_dag(t.node, indent + 1)\nend\n\n# 4. Implement utilities\ncount_steps(t::Timeout) = count_steps(t.node)\nsteps(t::Timeout) = steps(t.node)","category":"section"},{"location":"development/#Custom-Operator","page":"Development","title":"Custom Operator","text":"import Base: ^\n\n# Node^3 means repeat 3 times\n^(node::AbstractNode, n::Int) = Repeat(node, n)","category":"section"},{"location":"development/#Custom-Step-Executor","page":"Development","title":"Custom Step Executor","text":"Extend execute for new work types:\n\nstruct HTTPGet\n    url::String\nend\n\nfunction SimplePipelines.execute(step::Step{HTTPGet})\n    start = time()\n    resp = HTTP.get(step.work.url)\n    return StepResult(step, resp.status == 200, time() - start, String(resp.body))\nend\n\n# Usage: Step(:fetch, HTTPGet(\"https://api.example.com\"))","category":"section"},{"location":"development/#Type-Stability-Rules","page":"Development","title":"Type Stability Rules","text":"Use parametric types: struct MyNode{T<:AbstractNode} not children::Vector{AbstractNode}\nUse tuple recursion for heterogeneous collections\nDispatch on types, don't check with isa","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"This page shows pipeline patterns in order of increasing complexity. Each example has a flow diagram, a short goal, and runnable code.\n\nContents\n\nBasics — sequential, parallel, Julia + shell\nControl flow — retry, fallback, branching\nComplex DAGs — multi-stage parallel, robust pipeline\nBioinformatics — immune repertoire (single & multi-donor), variant calling\n\n","category":"section"},{"location":"examples/#1.-Basics","page":"Examples","title":"1. Basics","text":"","category":"section"},{"location":"examples/#1.1-Basic-Pipeline","page":"Examples","title":"1.1 Basic Pipeline","text":"Flow: Three steps in sequence.\n\n  download  ──►  process  ──►  upload\n\nGoal: Create a file, process it, then copy the result (runnable without network).\n\nusing SimplePipelines\n\ndownload = @step download = sh\"(echo line3; echo line1; echo line2) > data.txt\"\nprocess = @step process = sh\"sort data.txt > sorted.txt\"\nupload = @step upload = sh\"cp sorted.txt uploaded.txt\"\n\npipeline = download >> process >> upload\nrun(pipeline)\n\n","category":"section"},{"location":"examples/#1.2-Parallel-Processing","page":"Examples","title":"1.2 Parallel Processing","text":"Flow: Three steps run in parallel, then one step merges.\n\n       ┌── file_a ──┐\n       ├── file_b ──┼──►  archive\n       └── file_c ──┘\n\nGoal: Create three files, compress them concurrently, then archive (runnable as-is).\n\nusing SimplePipelines\n\nfile_a = @step a = sh\"echo content_a > file_a.txt && gzip -k file_a.txt\"\nfile_b = @step b = sh\"echo content_b > file_b.txt && gzip -k file_b.txt\"\nfile_c = @step c = sh\"echo content_c > file_c.txt && gzip -k file_c.txt\"\narchive = @step archive = sh\"tar -cvf archive.tar file_a.txt.gz file_b.txt.gz file_c.txt.gz\"\n\npipeline = (file_a & file_b & file_c) >> archive\nrun(pipeline)\n\n","category":"section"},{"location":"examples/#1.3-Julia-Shell","page":"Examples","title":"1.3 Julia + Shell","text":"Flow: Julia step → shell step → Julia step.\n\n  generate  ──►  process  ──►  report\n   (Julia)       (shell)      (Julia)\n\nGoal: Generate data in Julia, run a shell tool, then summarize in Julia.\n\nusing SimplePipelines\nusing DelimitedFiles\n\ngenerate = @step generate = () -> begin\n    data = rand(100, 10)\n    writedlm(\"matrix.csv\", data, ',')\n    return \"Generated $(size(data)) matrix\"\nend\n\nprocess = @step process = sh\"wc -l matrix.csv\"\n\nreport = @step report = () -> begin\n    lines = read(\"matrix.csv\", String)\n    nrows = count(==('\\n'), lines)\n    return \"Matrix has $nrows rows\"\nend\n\npipeline = generate >> process >> report\nrun(pipeline)\n\n","category":"section"},{"location":"examples/#2.-Control-flow","page":"Examples","title":"2. Control flow","text":"","category":"section"},{"location":"examples/#2.1-Retry-and-Fallback","page":"Examples","title":"2.1 Retry and Fallback","text":"Flow (retry then continue): Run a step up to N times, then continue.\n\n  [fetch^3]  ──►  process\n   (retry)\n\nFlow (fallback): If primary fails, run fallback.\n\n   primary  ──►  (on success)  result\n      │\n      └──►  (on failure)  fallback  ──►  result\n\nGoal: Retry flaky fetch; use fallback when primary step fails; combine retry and fallback.\n\nusing SimplePipelines\n\n# Retry then continue (fetch creates file; no network)\nfetch = @step fetch = sh\"echo '{\\\"x\\\":1}' > data.json\"\nprocess = @step process = sh\"wc -c data.json\"\npipeline = Retry(fetch, 3, delay=0.1) >> process\n\n# Fallback (create data.csv first so both branches have input)\nrun(@step setup = sh\"(echo 'a,b'; echo '1,2') > data.csv\", verbose=false)\nfast = @step fast = sh\"sort data.csv > sorted.csv\"\nslow = @step slow = sh\"cat data.csv > sorted.csv\"\npipeline = fast | slow\n\n# Retry then fallback\npipeline = Retry(fast, 3) | slow\nrun(pipeline)\n\n","category":"section"},{"location":"examples/#2.2-Conditional-Branching","page":"Examples","title":"2.2 Conditional Branching","text":"Flow: One of two branches runs based on a condition.\n\n            ┌── if true  ──►  branch_a\n  condition ─┤\n            └── if false ──►  branch_b\n\nGoal: Choose processing path by file size or environment (e.g. DEBUG).\n\nusing SimplePipelines\n\n# Create data.csv so the branch condition has a file to check\nrun(@step setup = sh\"(echo 'a,b'; echo '1,2'; echo '3,4') > data.csv\", verbose=false)\n\n# By file size\nsmall_pipeline = @step small = sh\"head -n 1000 data.csv > sample.csv\"\nlarge_pipeline = @step decompress = sh\"gunzip -c data.csv.gz > data.csv\" >> @step process = sh\"split -l 10000 data.csv chunk_\"\npipeline = Branch(\n    () -> filesize(\"data.csv\") < 100_000_000,\n    small_pipeline,\n    large_pipeline\n)\nrun(pipeline)\n\n# By environment\ndebug_steps = @step debug = sh\"echo 'debug mode'\"\nprod_steps = @step prod = sh\"echo 'production'\"\npipeline = Branch(() -> get(ENV, \"DEBUG\", \"0\") == \"1\", debug_steps, prod_steps)\nrun(pipeline)\n\n","category":"section"},{"location":"examples/#3.-Complex-DAGs","page":"Examples","title":"3. Complex DAGs","text":"","category":"section"},{"location":"examples/#3.1-Multi-stage-Parallel","page":"Examples","title":"3.1 Multi-stage Parallel","text":"Flow: Two parallel fetch+transform branches, then merge, analyze, then report and archive in parallel.\n\n  ┌── fetch_db   ──►  transform_db  ──┐\n  │                                    ├──►  merge  ──►  analyze  ──►  ┌── report\n  └── fetch_files ──►  transform_files ─┘                               └── archive\n\nGoal: Fetch from two sources in parallel (here: create two files), process each, merge, analyze, then report and archive (runnable without network).\n\nusing SimplePipelines\n\nfetch_db = @step db = sh\"echo '{\\\"db\\\":1}' > db_data.json\"\nfetch_files = @step files = sh\"echo 'local_data' > local_data.txt\"\n\ntransform_db = @step transform_db = sh\"wc -c db_data.json > db_size.txt\"\ntransform_files = @step transform_files = sh\"wc -c local_data.txt > files_size.txt\"\n\nmerge = @step merge = sh\"cat db_size.txt files_size.txt > merged.txt\"\nanalyze = @step analyze = sh\"wc -l merged.txt > results.txt\"\n\nreport = @step report = sh\"cat results.txt\"\narchive = @step archive = sh\"gzip -c merged.txt > results.tar.gz\"\n\ndb_branch = fetch_db >> transform_db\nfiles_branch = fetch_files >> transform_files\npipeline = (db_branch & files_branch) >> merge >> analyze >> (report & archive)\n\nrun(pipeline)\n\n","category":"section"},{"location":"examples/#3.2-Robust-Pipeline-(all-features)","page":"Examples","title":"3.2 Robust Pipeline (all features)","text":"Flow: Retry+fallback fetch → conditional process (small vs full) → report and notify in parallel (notify with retry).\n\n  [primary^3 | backup]  ──►  [small? quick : full]  ──►  report\n                                                      └── notify^2\n\nGoal: Fetch with retry and fallback; branch on file size; run report and notify in parallel (runnable without network).\n\nusing SimplePipelines\n\nprimary_source = @step primary = sh\"echo '{\\\"status\\\":\\\"ok\\\"}' > data.json\"\nbackup_source = @step backup = sh\"echo '{\\\"status\\\":\\\"fallback\\\"}' > data.json\"\nfetch = Retry(primary_source, 3, delay=0.1) | backup_source\n\nquick_process = @step quick = sh\"wc -c data.json > output.txt\"\nfull_process = @step parse = sh\"wc -l data.json > output.txt\" >> @step validate = sh\"wc -c output.txt >> output.txt\"\n\nprocess = Branch(\n    () -> filesize(\"data.json\") < 1_000_000,\n    quick_process,\n    full_process\n)\n\nreport = @step report = sh\"cat output.txt\"\nnotify = @step notify = sh\"echo 'Pipeline done'\"\n\npipeline = fetch >> process >> (report & Retry(notify, 2))\nrun(Pipeline(pipeline, name=\"Robust ETL\"))\n\n","category":"section"},{"location":"examples/#4.-Bioinformatics","page":"Examples","title":"4. Bioinformatics","text":"Multiple donors / samples: Use ForEach with a {wildcard} pattern to automatically discover files and create parallel branches. The wildcard values propagate via normal Julia $() interpolation.\n\nNote: Examples below use real tool names (PEAR, IgBLAST, BWA, etc.). They are runnable only if those tools are installed and input files exist; otherwise use echo placeholders as in the basic examples.","category":"section"},{"location":"examples/#4.1-Immune-Repertoire-(single-donor)","page":"Examples","title":"4.1 Immune Repertoire (single donor)","text":"Flow: Paired-end FASTQ → merge (PEAR) → FASTQ→FASTA → IgBLAST (V/D/J) → Julia filter by identity.\n\n  R1.fastq ──┐\n             ├──►  pear  ──►  to_fasta  ──►  igblast  ──►  filter_identity  ──►  igblast_filtered.tsv\n  R2.fastq ──┘\n\nGoal: Merge paired-end reads, run IgBLAST with V/D/J references, filter rows by v_identity and j_identity > 90% in Julia.\n\nusing SimplePipelines\nusing DelimitedFiles\n\npear = @step pear = sh\"pear -f R1.fastq -r R2.fastq -o merged\"\nto_fasta = @step to_fasta = sh\"seqtk seq -A merged.assembled.fastq > merged.assembled.fasta\"\nigblast = @step igblast = sh\"igblastn -query merged.assembled.fasta -germline_db_V V.fasta -germline_db_D D.fasta -germline_db_J J.fasta -outfmt 7 -out igblast.tsv\"\n\nfilter_identity = @step filter_identity = () -> begin\n    data, header_row = readdlm(\"igblast.tsv\", '\\t', header=true)\n    header = vec(header_row)\n    v_idx = findfirst(isequal(\"v_identity\"), header)\n    j_idx = findfirst(isequal(\"j_identity\"), header)\n    to_float(x) = something(tryparse(Float64, string(x)), 0.0)\n    filtered = [r for r in eachrow(data) if to_float(r[v_idx]) > 90.0 && to_float(r[j_idx]) > 90.0]\n    writedlm(\"igblast_filtered.tsv\", vcat(header', filtered), '\\t')\n    return \"Filtered $(length(filtered)) sequences\"\nend\n\npipeline = pear >> to_fasta >> igblast >> filter_identity\nrun(Pipeline(pipeline, name=\"Immune Repertoire\"))\n\n","category":"section"},{"location":"examples/#4.2-Immune-Repertoire-(multiple-donors)","page":"Examples","title":"4.2 Immune Repertoire (multiple donors)","text":"Flow: ForEach discovers files matching pattern → one parallel branch per donor.\n\n  ForEach(\"fastq/{donor}_R1.fq.gz\")\n       │\n       ├── donor1: pear → fasta → igblast  ──┐\n       ├── donor2: pear → fasta → igblast  ──┼── (parallel)\n       └── donorN: pear → fasta → igblast  ──┘\n\nGoal: Run the same pipeline for every donor; files discovered automatically.\n\nusing SimplePipelines\n\n# ForEach: pattern discovery + parallel branches in one construct\npipeline = ForEach(\"fastq/{donor}_R1.fq.gz\") do donor\n    sh(\"pear -f fastq/$(donor)_R1.fq.gz -r fastq/$(donor)_R2.fq.gz -o $(donor)_merged\") >>\n    sh(\"seqtk seq -A $(donor)_merged.assembled.fastq > $(donor).fasta\") >>\n    sh(\"igblastn -query $(donor).fasta -germline_db_V V.fasta -germline_db_D D.fasta -germline_db_J J.fasta -outfmt 7 -out $(donor)_igblast.tsv\")\nend\n\nrun(pipeline)\n\n","category":"section"},{"location":"examples/#4.3-Variant-Calling","page":"Examples","title":"4.3 Variant Calling","text":"Flow: Paired-end reads → FastQC → trim → BWA (GRCh38) → sort/index → bcftools call → index → filter.\n\n  R1.fq.gz ──┐\n             ├──►  fastqc  ──►  trim  ──►  align  ──►  index  ──►  call  ──►  index_vcf  ──►  filter_vcf\n  R2.fq.gz ──┘\n\nGoal: QC, trim, align to GRCh38, call variants with bcftools, filter by quality.\n\nusing SimplePipelines\n\nfastqc = @step fastqc = sh\"fastqc -o qc/ R1.fq.gz R2.fq.gz\"\ntrim = @step trim = sh\"trimmomatic PE R1.fq.gz R2.fq.gz R1_trimmed.fq.gz R1_unpaired.fq.gz R2_trimmed.fq.gz R2_unpaired.fq.gz ILLUMINACLIP:adapters.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\"\n\nalign = @step align = sh\"bwa mem -t 8 GRCh38.fa R1_trimmed.fq.gz R2_trimmed.fq.gz | samtools sort -@ 4 -o aligned.bam -\"\nindex = @step index = sh\"samtools index aligned.bam\"\n\ncall = @step call = sh\"bcftools mpileup -f GRCh38.fa aligned.bam | bcftools call -mv -Oz -o variants.vcf.gz\"\nindex_vcf = @step index_vcf = sh\"bcftools index variants.vcf.gz\"\nfilter_vcf = @step filter_vcf = sh\"bcftools filter -i 'QUAL>=20' variants.vcf.gz -Oz -o filtered.vcf.gz\"\n\npipeline = fastqc >> trim >> align >> index >> call >> index_vcf >> filter_vcf\nrun(Pipeline(pipeline, name=\"Variant Calling\"))","category":"section"},{"location":"#SimplePipelines.jl","page":"Home","title":"SimplePipelines.jl","text":"Minimal, type-stable DAG pipelines for Julia","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"SimplePipelines.jl lets you define and execute directed acyclic graph (DAG) pipelines using intuitive operators.","category":"section"},{"location":"#Steps","page":"Home","title":"Steps","text":"Syntax Description\n@step name = sh\"cmd\" Shell command\n@step name = sh\"cmd > file\" Shell with redirection/pipes\n@step name = () -> expr Julia function","category":"section"},{"location":"#Operators","page":"Home","title":"Operators","text":"Operator Description Example\n>> Sequential a >> b >> c\n& Parallel a & b & c\n| Fallback a | b (b if a fails)\n^n Retry a^3 (up to 3 times)","category":"section"},{"location":"#Control-Flow","page":"Home","title":"Control Flow","text":"Function Description\nTimeout(a, secs) Fail if exceeds time\nBranch(cond, a, b) Conditional execution\nMap(f, items) Fan-out over collection\nReduce(f, a & b) Combine parallel outputs\nForEach(pattern) do ... Discover files by pattern\nRetry(a, n, delay=d) Retry with delay","category":"section"},{"location":"#Execution-and-Results","page":"Home","title":"Execution & Results","text":"Function / Field Description\nrun(p) / `p > run`\nrun(p, verbose=false) Run silently\nrun(p, dry_run=true) Preview only\nresults[i].success Did step succeed?\nresults[i].duration Time in seconds\nresults[i].output Output or error","category":"section"},{"location":"#Utilities","page":"Home","title":"Utilities","text":"Function Description\nprint_dag(node) Visualize structure\ncount_steps(node) Count steps\nsteps(node) Get all steps","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"using SimplePipelines\n\n# Simple sequence\npipeline = sh\"echo 'step 1'\" >> sh\"echo 'step 2'\" >> sh\"echo 'step 3'\"\nrun(pipeline)\n\n# Parallel branches that merge\npipeline = (sh\"process A\" & sh\"process B\") >> sh\"merge\"\nrun(pipeline)","category":"section"},{"location":"#DAG-Patterns","page":"Home","title":"DAG Patterns","text":"","category":"section"},{"location":"#Diamond-(fork-join)","page":"Home","title":"Diamond (fork-join)","text":"       ┌── step_b ──┐\nstep_a─┤            ├── step_d\n       └── step_c ──┘\n\nstep_a = @step a = sh\"fetch\"\nstep_b = @step b = sh\"analyze_a\"\nstep_c = @step c = sh\"analyze_b\"\nstep_d = @step d = sh\"report\"\npipeline = step_a >> (step_b & step_c) >> step_d","category":"section"},{"location":"#Multi-stage-parallel","page":"Home","title":"Multi-stage parallel","text":"       ┌─ b ─┐     ┌─ e ─┐\n    a ─┤     ├─ d ─┤     ├─ g\n       └─ c ─┘     └─ f ─┘\n\na = @step a = sh\"stage_a\"\nb = @step b = sh\"stage_b\"\nc = @step c = sh\"stage_c\"\nd = @step d = sh\"stage_d\"\ne = @step e = sh\"stage_e\"\nf = @step f = sh\"stage_f\"\ng = @step g = sh\"stage_g\"\npipeline = a >> (b & c) >> d >> (e & f) >> g","category":"section"},{"location":"#Independent-branches-merging","page":"Home","title":"Independent branches merging","text":"    ┌─ a ── b ─┐\n    │          │\n    ├─ c ── d ─┼── merge\n    │          │\n    └─ e ── f ─┘\n\na = @step a = sh\"branch1_a\"\nb = @step b = sh\"branch1_b\"\nc = @step c = sh\"branch2_a\"\nd = @step d = sh\"branch2_b\"\ne = @step e = sh\"branch3_a\"\nf = @step f = sh\"branch3_b\"\nmerge = @step merge = sh\"merge\"\nbranch1 = a >> b\nbranch2 = c >> d\nbranch3 = e >> f\npipeline = (branch1 & branch2 & branch3) >> merge","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Intuitive operators - >> sequence, & parallel, | fallback, ^ retry\nControl flow - Timeout, Branch, Map for complex workflows\nType-stable - Zero runtime type checks, full compile-time specialization\nMinimal overhead - @inline functions and tuple recursion\nComposable - All operators work together seamlessly\nUnified interface - Shell commands and Julia functions compose seamlessly","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"Pages = [\"tutorial.md\", \"examples.md\", \"api.md\", \"design.md\", \"development.md\"]\nDepth = 2","category":"section"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#Steps","page":"Tutorial","title":"Steps","text":"A Step is the basic unit of work—either a shell command or Julia function.","category":"section"},{"location":"tutorial/#Shell-Commands","page":"Tutorial","title":"Shell Commands","text":"using SimplePipelines\n\n# Direct command (anonymous step)\nstep = @step sh\"samtools sort input.bam\"\n\n# Named step\nstep = @step sort = sh\"samtools sort input.bam\"\n\n# Shell features (>, |, &&) - use sh\"...\"\nstep = @step sort = sh\"sort data.txt | uniq > sorted.txt\"","category":"section"},{"location":"tutorial/#Julia-Functions","page":"Tutorial","title":"Julia Functions","text":"# Anonymous function step\nstep = @step () -> process_data()\n\n# Named function step\nstep = @step analyze = () -> run_analysis(\"data.csv\")","category":"section"},{"location":"tutorial/#File-Dependencies-(Optional)","page":"Tutorial","title":"File Dependencies (Optional)","text":"Track input/output files for validation:\n\n@step align(\"reads.fq\" => \"aligned.bam\") = sh\"bwa mem ref.fa reads.fq > aligned.bam\"  # sh\"...\" for redirection","category":"section"},{"location":"tutorial/#Sequential-Execution:","page":"Tutorial","title":"Sequential Execution: >>","text":"The >> operator chains steps—each waits for the previous to complete:\n\n# Chain commands directly (anonymous steps)\npipeline = sh\"download data.txt\" >> sh\"process data.txt\" >> sh\"upload results.txt\"\n\n# Or define named steps, then chain\nstep_a = @step step_a = sh\"download data.txt\"\nstep_b = @step step_b = sh\"process data.txt\"\nstep_c = @step step_c = sh\"upload results.txt\"\npipeline = step_a >> step_b >> step_c","category":"section"},{"location":"tutorial/#Parallel-Execution:-and","page":"Tutorial","title":"Parallel Execution: &","text":"The & operator groups steps to run concurrently:\n\n# Parallel steps (anonymous)\nparallel = sh\"task_a\" & sh\"task_b\" & sh\"task_c\"\n\n# Named steps in parallel, then merge\nsample_1 = @step s1 = sh\"process sample1\"\nsample_2 = @step s2 = sh\"process sample2\"\nsample_3 = @step s3 = sh\"process sample3\"\nmerge_results = @step merge = sh\"merge outputs\"\npipeline = (sample_1 & sample_2 & sample_3) >> merge_results","category":"section"},{"location":"tutorial/#Complex-DAGs","page":"Tutorial","title":"Complex DAGs","text":"Combine >> and & for arbitrary graphs.","category":"section"},{"location":"tutorial/#Diamond-Pattern","page":"Tutorial","title":"Diamond Pattern","text":"       ┌── analyze_a ──┐\n fetch─┤               ├── report\n       └── analyze_b ──┘\n\nfetch = @step fetch = sh\"echo 'a,b\\n1,2' > data.csv\"\nanalyze_a = @step a = sh\"wc -l data.csv\"\nanalyze_b = @step b = sh\"wc -c data.csv\"\nreport = @step report = () -> \"done\"\n\npipeline = fetch >> (analyze_a & analyze_b) >> report\nrun(pipeline)","category":"section"},{"location":"tutorial/#Multi-Stage-Parallel","page":"Tutorial","title":"Multi-Stage Parallel","text":"For graphs with multiple fork-join points, compose in stages:\n\n     ┌─ b ─┐     ┌─ e ─┐\n  a ─┤     ├─ d ─┤     ├─ g\n     └─ c ─┘     └─ f ─┘\n\na = @step a = sh\"step_a\"\nb = @step b = sh\"step_b\"\nc = @step c = sh\"step_c\"\nd = @step d = sh\"step_d\"\ne = @step e = sh\"step_e\"\nf = @step f = sh\"step_f\"\ng = @step g = sh\"step_g\"\n\npipeline = a >> (b & c) >> d >> (e & f) >> g","category":"section"},{"location":"tutorial/#Independent-Branches","page":"Tutorial","title":"Independent Branches","text":"Process independent pipelines in parallel, then merge:\n\n  ┌─ fetch_a >> process_a ─┐\n  │                        │\n  ├─ fetch_b >> process_b ─┼── merge\n  │                        │\n  └─ fetch_c >> process_c ─┘\n\nfetch_a = @step fetch_a = sh\"fetch sample_a\"\nprocess_a = @step process_a = sh\"process sample_a\"\nfetch_b = @step fetch_b = sh\"fetch sample_b\"\nprocess_b = @step process_b = sh\"process sample_b\"\nfetch_c = @step fetch_c = sh\"fetch sample_c\"\nprocess_c = @step process_c = sh\"process sample_c\"\nmerge = @step merge = sh\"merge results\"\n\nbranch_a = fetch_a >> process_a\nbranch_b = fetch_b >> process_b\nbranch_c = fetch_c >> process_c\npipeline = (branch_a & branch_b & branch_c) >> merge\n\nThis pattern is common for processing multiple samples/files independently before combining results.","category":"section"},{"location":"tutorial/#Fallback:","page":"Tutorial","title":"Fallback: |","text":"The | operator provides fallback behavior—if the primary fails, run the fallback:\n\n# If fast method fails, use slow method\nfast_method = @step fast = sh\"fast_tool input.txt\"\nslow_method = @step slow = sh\"slow_tool input.txt\"\npipeline = fast_method | slow_method\n\n# Chain multiple fallbacks\nmethod_a = @step a = sh\"method_a input\"\nmethod_b = @step b = sh\"method_b input\"\nmethod_c = @step c = sh\"method_c input\"\npipeline = method_a | method_b | method_c","category":"section"},{"location":"tutorial/#Retry:-or-Retry()","page":"Tutorial","title":"Retry: ^ or Retry()","text":"Retry a node up to N times on failure:\n\n# Using ^ operator (concise) – retry flaky step up to 3 times\nflaky_api_call = @step api = sh\"echo 'mock response'\"\npipeline = flaky_api_call^3\n\n# Using Retry() with delay between attempts\nnetwork_request = @step fetch = sh\"echo 'data'\"\npipeline = Retry(network_request, 5, delay=2.0)\n\n# Combine with fallback\nprimary = @step primary = sh\"echo primary\"\nfallback = @step fallback = sh\"echo fallback\"\npipeline = primary^3 | fallback","category":"section"},{"location":"tutorial/#Branch-(Conditional)","page":"Tutorial","title":"Branch (Conditional)","text":"Execute different branches based on a runtime condition:\n\n# Branch based on file size\nlarge_file_pipeline = @step large = sh\"process_large data.txt\"\nsmall_file_pipeline = @step small = sh\"process_small data.txt\"\npipeline = Branch(\n    () -> filesize(\"data.txt\") > 1_000_000,\n    large_file_pipeline,\n    small_file_pipeline\n)\n\n# Branch based on environment\ndebug_steps = @step debug = sh\"run with verbose logging\"\nnormal_steps = @step normal = sh\"run quietly\"\npipeline = Branch(\n    () -> haskey(ENV, \"DEBUG\"),\n    debug_steps,\n    normal_steps\n)","category":"section"},{"location":"tutorial/#Timeout","page":"Tutorial","title":"Timeout","text":"Fail if a node exceeds a time limit:\n\n# 30 second timeout\nlong_running_step = @step long = sh\"sleep 1\"\npipeline = Timeout(long_running_step, 30.0)\n\n# Combine with retry and fallback\napi_call = @step api = sh\"echo result\"\nbackup = @step backup = sh\"echo fallback\"\npipeline = Timeout(api_call, 5.0)^3 | backup","category":"section"},{"location":"tutorial/#Map-(Fan-out)","page":"Tutorial","title":"Map (Fan-out)","text":"Apply a function to each item, creating parallel steps:\n\n# Process files in parallel (list supplied in code)\nsamples = [\"sample_A\", \"sample_B\", \"sample_C\"]\npipeline = Map(samples) do s\n    Step(Symbol(\"process_\", s), sh(\"analyze $s.fastq\"))\nend >> merge_results","category":"section"},{"location":"tutorial/#ForEach-(Pattern-based-discovery)","page":"Tutorial","title":"ForEach (Pattern-based discovery)","text":"Discover files by pattern, create parallel branches automatically. ForEach scans the filesystem, so matching files must exist. Example with a temp dir:\n\n# Create dummy files so ForEach can find matches (run in a temp dir)\ncd(mktempdir()) do\n    write(\"s1.fastq\", \"\"); write(\"s2.fastq\", \"\")\n    mkpath(\"fastq\"); write(\"fastq/s1_R1.fq.gz\", \"\"); write(\"fastq/s2_R1.fq.gz\", \"\")\n    mkpath(\"data/p1\"); write(\"data/p1/s1.csv\", \"\")\n\n    # Single step per file - use sh(\"...\") for interpolation\n    ForEach(\"{sample}.fastq\") do sample\n        sh(\"process $(sample).fastq\")\n    end\n\n    # Multi-step per file - chain with >>\n    ForEach(\"fastq/{sample}_R1.fq.gz\") do sample\n        sh(\"pear $(sample)_R1 $(sample)_R2\") >> sh(\"analyze $(sample)\")\n    end\n\n    # Multiple wildcards\n    ForEach(\"data/{project}/{sample}.csv\") do project, sample\n        sh(\"process $(project)/$(sample).csv\")\n    end\n\n    # Chain with downstream merge\n    ForEach(\"{id}.fastq\") do id\n        sh(\"align $(id).fastq\")\n    end >> @step merge = sh\"merge *.bam\"\nend","category":"section"},{"location":"tutorial/#Reduce-(Combine)","page":"Tutorial","title":"Reduce (Combine)","text":"Collect outputs from parallel steps and combine them:\n\n# Combine parallel outputs with a function (define steps first)\nanalyze_a = @step a = sh\"echo result_a\"\nanalyze_b = @step b = sh\"echo result_b\"\npipeline = Reduce(analyze_a & analyze_b) do outputs\n    join(outputs, \"\\n\")\nend\n\n# Using a named function (define reducer and steps first)\ncombine_results(outputs) = join(outputs, \"\\n\")\nstep_a = @step a = sh\"echo result_a\"\nstep_b = @step b = sh\"echo result_b\"\nstep_c = @step c = sh\"echo result_c\"\npipeline = Reduce(combine_results, step_a & step_b & step_c)\n\n# In a pipeline: fetch -> parallel analysis -> reduce -> report\nmerge_outputs(outputs) = join(outputs, \"\\n\")\nfetch = @step fetch = sh\"echo data\"\nanalyze_a = @step a = sh\"wc -c\"\nanalyze_b = @step b = sh\"wc -l\"\nreport = @step report = sh\"echo done\"\npipeline = fetch >> Reduce(merge_outputs, analyze_a & analyze_b) >> report\n\nThe reducer function receives a Vector{String} of outputs from all successful parallel steps.","category":"section"},{"location":"tutorial/#Running-Pipelines","page":"Tutorial","title":"Running Pipelines","text":"Use run(pipeline) or pipeline |> run:\n\n# Basic execution\nresults = run(pipeline)\n\n# Silent (no progress output)\nresults = run(pipeline, verbose=false)\n\n# Dry run (preview structure)\nrun(pipeline, dry_run=true)\n\n# Named pipeline\nstep_a = @step a = sh\"first\"\nstep_b = @step b = sh\"second\"\np = Pipeline(step_a >> step_b, name=\"My Workflow\")\nrun(p)","category":"section"},{"location":"tutorial/#Checking-Results","page":"Tutorial","title":"Checking Results","text":"results = run(pipeline)\n\nfor r in results\n    if r.success\n        println(\"$(r.step.name): completed in $(r.duration)s\")\n    else\n        println(\"$(r.step.name): FAILED - $(r.output)\")\n    end\nend\n\n# Check overall success\nall_ok = all(r -> r.success, results)","category":"section"},{"location":"tutorial/#Mixing-Shell-and-Julia","page":"Tutorial","title":"Mixing Shell and Julia","text":"Shell commands and Julia functions compose seamlessly:\n\n# Julia: prepare data (e.g. filter non-empty lines)\nprep = @step prep = () -> begin\n    raw = read(\"raw.csv\", String)\n    cleaned = filter(line -> !isempty(strip(line)), split(raw, '\\n'))\n    write(\"clean.csv\", join(cleaned, '\\n'))\n    return \"Wrote $(length(cleaned)) lines\"\nend\n\n# Shell: run external tool\nexternal = @step tool = sh\"wc -l clean.csv > result.txt\"  # sh\"...\" for redirection\n\n# Julia: postprocess\npost = @step post = () -> begin\n    n = parse(Int, split(read(\"result.txt\", String))[1])\n    return \"Line count: $n\"\nend\n\npipeline = prep >> external >> post\nrun(pipeline)","category":"section"},{"location":"tutorial/#Utilities","page":"Tutorial","title":"Utilities","text":"# Count steps in a pipeline\nn = count_steps(pipeline)\n\n# Get all steps as a vector\nall_steps = steps(pipeline)\n\n# Print DAG structure\nprint_dag(pipeline)","category":"section"}]
}
