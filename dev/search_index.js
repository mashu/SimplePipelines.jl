var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Types","page":"API Reference","title":"Types","text":"","category":"section"},{"location":"api/#Macros","page":"API Reference","title":"Macros","text":"","category":"section"},{"location":"api/#Operators","page":"API Reference","title":"Operators","text":"","category":"section"},{"location":"api/#Functions","page":"API Reference","title":"Functions","text":"","category":"section"},{"location":"api/#Execution","page":"API Reference","title":"Execution","text":"","category":"section"},{"location":"api/#Utilities","page":"API Reference","title":"Utilities","text":"","category":"section"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/#SimplePipelines.AbstractNode","page":"API Reference","title":"SimplePipelines.AbstractNode","text":"AbstractNode\n\nBase type for all pipeline nodes. Subtypes are:\n\nStep: A single unit of work\nSequence: Nodes that run in order\nParallel: Nodes that run concurrently\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Step","page":"API Reference","title":"SimplePipelines.Step","text":"Step{F}\n\nA single unit of work in the pipeline.\n\nType Parameters\n\nF: The type of work (e.g., Cmd for shell commands, or a Function subtype)\n\nFields\n\nname::Symbol: Identifier for the step\nwork::F: The work to execute\ninputs::Vector{String}: Input file dependencies (optional)\noutputs::Vector{String}: Output files produced (optional)\n\nExamples\n\n# Shell command\nStep(:align, `bwa mem ref.fa reads.fq`)\n\n# Julia function\nStep(:qc, () -> run_quality_control())\n\n# With file dependencies\nStep(:variant_call, `bcftools call -m aligned.bam`, [\"aligned.bam\"], [\"variants.vcf\"])\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Sequence","page":"API Reference","title":"SimplePipelines.Sequence","text":"Sequence{T<:Tuple}\n\nA sequence of nodes that execute in order. The type parameter T captures the exact tuple type for full type stability.\n\nExamples\n\n# Created via >> operator\nalign >> sort >> index\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Parallel","page":"API Reference","title":"SimplePipelines.Parallel","text":"Parallel{T<:Tuple}\n\nNodes that execute concurrently. The type parameter T captures the exact tuple type for full type stability.\n\nExamples\n\n# Created via & operator\n(sample_a & sample_b & sample_c) >> merge\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Retry","page":"API Reference","title":"SimplePipelines.Retry","text":"Retry{N<:AbstractNode}\n\nRetry a node up to max_attempts times on failure, with optional delay between attempts.\n\nExamples\n\n# Retry up to 3 times\nRetry(flaky_step, 3)\n\n# Retry with delay between attempts\nRetry(api_call, 5, delay=2.0)\n\n# Using | operator for simple fallback (try once, then fallback)\nrisky_step | safe_fallback\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Fallback","page":"API Reference","title":"SimplePipelines.Fallback","text":"Fallback{A<:AbstractNode, B<:AbstractNode}\n\nTry the primary node; if it fails, run the fallback node. Created with the | operator.\n\nExamples\n\n# If fast_method fails, use slow_method\nfast_method | slow_method\n\n# Chain multiple fallbacks\nmethod_a | method_b | method_c\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Branch","page":"API Reference","title":"SimplePipelines.Branch","text":"Branch{C<:Function, T<:AbstractNode, F<:AbstractNode}\n\nConditional execution: run if_true when condition() returns true, otherwise if_false.\n\nExamples\n\n# Branch based on file size\nBranch(() -> filesize(\"data.txt\") > 1_000_000, large_file_pipeline, small_file_pipeline)\n\n# Branch based on environment\nBranch(() -> haskey(ENV, \"DEBUG\"), debug_steps, normal_steps)\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Timeout","page":"API Reference","title":"SimplePipelines.Timeout","text":"Timeout{N<:AbstractNode}\n\nFail if a node doesn't complete within the specified time.\n\nExamples\n\n# 30 second timeout\nTimeout(long_step, 30.0)\n\n# Combine with retry and fallback\nRetry(Timeout(primary, 10.0), 3) | fallback\n\n# Using ^ operator: step^3 = retry 3 times\nTimeout(api_call, 5.0)^3 | backup\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Reduce","page":"API Reference","title":"SimplePipelines.Reduce","text":"Reduce{F<:Function, N<:AbstractNode}\n\nRun a node (typically Parallel) and combine outputs with a reducing function. The function receives a vector of outputs from successful steps.\n\nExamples\n\n# Combine parallel outputs\nReduce(a & b & c) do outputs\n    join(outputs, \"\\n\")\nend\n\n# With a named function\nReduce(combine_results, process_a & process_b)\n\n# In a pipeline\nfetch >> Reduce(merge, analyze_a & analyze_b) >> report\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.Pipeline","page":"API Reference","title":"SimplePipelines.Pipeline","text":"Pipeline{N<:AbstractNode}\n\nA complete pipeline ready for execution.\n\nFields\n\nroot::N: The root node of the pipeline DAG\nname::String: Human-readable name for the pipeline\n\nExamples\n\n# Create from composed nodes\np = Pipeline(align >> sort >> index, name=\"alignment\")\n\n# Or wrap multiple nodes (creates a Sequence)\np = Pipeline(step1, step2, step3)\n\n\n\n\n\n","category":"type"},{"location":"api/#SimplePipelines.@step","page":"API Reference","title":"SimplePipelines.@step","text":"@step expr\n@step name = expr\n@step name(inputs => outputs) = expr\n\nCreate a Step with optional name and file dependencies.\n\nExamples\n\n# Anonymous step\n@step `fastqc sample.fq`\n\n# Named step\n@step align = `bwa mem ref.fa reads.fq > aligned.sam`\n\n# With file dependencies (for dependency tracking)\n@step sort(\"aligned.sam\" => \"sorted.bam\") = `samtools sort aligned.sam`\n\n# Julia function\n@step qc_report = generate_multiqc_report\n\n\n\n\n\n","category":"macro"},{"location":"api/#Base.:>>","page":"API Reference","title":"Base.:>>","text":"a >> b\n\nCreate a Sequence where a completes before b starts.\n\nSupports chaining: a >> b >> c creates a single flattened sequence.\n\nExamples\n\n# Basic sequence\nfastqc >> trim >> align\n\n# Chain shell commands directly\n`fastqc raw.fq` >> `trimmomatic ...` >> `bwa mem ...`\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.:&","page":"API Reference","title":"Base.:&","text":"a & b\n\nCreate a Parallel where a and b run concurrently.\n\nSupports chaining: a & b & c creates a single parallel group.\n\nExamples\n\n# Process multiple samples in parallel\n(sample1 & sample2 & sample3) >> merge_results\n\n# Mix with sequences for complex DAGs\n(trim_a >> align_a) & (trim_b >> align_b) >> joint_call\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.:|","page":"API Reference","title":"Base.:|","text":"a | b\n\nCreate a Fallback: if a fails, run b.\n\nExamples\n\n# Try primary, fallback to secondary on failure\nprimary_method | fallback_method\n\n# Chain multiple fallbacks\nfast | medium | slow\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.:^","page":"API Reference","title":"Base.:^","text":"a^n\n\nCreate a Retry that attempts a up to n times.\n\nExamples\n\n# Retry step up to 3 times\nflaky_step^3\n\n# Combine with fallback\nflaky_step^3 | backup\n\n# With timeout\nTimeout(api_call, 5.0)^3\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.Map","page":"API Reference","title":"SimplePipelines.Map","text":"Map(f, items) -> Parallel\n\nApply function f to each item, creating parallel steps.\n\nExamples\n\n# Process files in parallel\nMap([\"a.txt\", \"b.txt\", \"c.txt\"]) do file\n    @step Symbol(file) = `process $file`\nend\n\n# With named steps\nsamples = [\"sample_A\", \"sample_B\", \"sample_C\"]\nMap(samples) do s\n    Step(Symbol(\"process_\", s), `analyze $s.fastq`)\nend >> merge_step\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.run_pipeline","page":"API Reference","title":"SimplePipelines.run_pipeline","text":"run_pipeline(p::Pipeline; verbose=true, dry_run=false)\nrun_pipeline(node::AbstractNode; verbose=true, dry_run=false)\n\nExecute a pipeline or node.\n\nArguments\n\nverbose::Bool=true: Print progress information\ndry_run::Bool=false: Show DAG structure without executing\n\nReturns\n\nVector{StepResult}: Results from all executed steps\n\nExamples\n\n# Run with progress output\nresults = run_pipeline(pipeline)\n\n# Silent execution\nresults = run_pipeline(pipeline, verbose=false)\n\n# Preview structure\nrun_pipeline(pipeline, dry_run=true)\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.count_steps","page":"API Reference","title":"SimplePipelines.count_steps","text":"count_steps(node::AbstractNode) -> Int\n\nCount total steps in a pipeline node.\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.steps","page":"API Reference","title":"SimplePipelines.steps","text":"steps(node::AbstractNode) -> Vector{Step}\n\nFlatten all steps from a pipeline node into a vector.\n\n\n\n\n\n","category":"function"},{"location":"api/#SimplePipelines.print_dag","page":"API Reference","title":"SimplePipelines.print_dag","text":"print_dag(node; indent=0)\n\nPrint the DAG structure of a pipeline node.\n\nExamples\n\npipeline = (a & b) >> c >> (d & e)\nprint_dag(pipeline)\n# Output:\n# Sequence:\n#   Parallel:\n#     a\n#     b\n#   c\n#   Parallel:\n#     d\n#     e\n\n\n\n\n\n","category":"function"},{"location":"design/#Design","page":"Design","title":"Design","text":"","category":"section"},{"location":"design/#Interface-Overview","page":"Design","title":"Interface Overview","text":"┌─────────────────────────────────────────────────────────────┐\n│                    SimplePipelines.jl                       │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  @step name = `command`     Create a shell step             │\n│  @step name = () -> ...     Create a Julia step             │\n│                                                             │\n│  a >> b                     Sequential: a then b            │\n│  a & b                      Parallel: a and b together      │\n│                                                             │\n│  run_pipeline(p)            Execute the pipeline            │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘","category":"section"},{"location":"design/#Type-Hierarchy","page":"Design","title":"Type Hierarchy","text":"AbstractNode\n    │\n    ├── Step{F}           Single unit of work\n    │                     F = Cmd | Function\n    │\n    ├── Sequence{T}       Sequential execution\n    │                     T = Tuple of nodes\n    │\n    └── Parallel{T}       Concurrent execution\n                          T = Tuple of nodes\n\nAll types are fully parametric—the compiler knows exact types at every level.","category":"section"},{"location":"design/#Composition-Model","page":"Design","title":"Composition Model","text":"# User writes:\n`echo a` >> (`echo b` & `echo c`) >> `echo d`\n\n# Becomes:\nSequence{Tuple{\n    Step{Cmd},\n    Parallel{Tuple{Step{Cmd}, Step{Cmd}}},\n    Step{Cmd}\n}}\n\nThe complete structure is encoded in the type, enabling full compile-time specialization.","category":"section"},{"location":"design/#Execution-Flow","page":"Design","title":"Execution Flow","text":"run_pipeline(Pipeline)\n       │\n       ▼\nrun_node(root, verbosity)  ─── dispatches on node type\n       │\n       ├─► Step:     execute(step) → StepResult\n       │\n       ├─► Sequence: run first node, then recurse on rest\n       │\n       └─► Parallel: @spawn all nodes, fetch all results","category":"section"},{"location":"design/#Key-Design-Decisions","page":"Design","title":"Key Design Decisions","text":"","category":"section"},{"location":"design/#1.-Tuples,-Not-Vectors","page":"Design","title":"1. Tuples, Not Vectors","text":"# ✗ Vector: type information lost\nSequence(nodes::Vector{AbstractNode})\n\n# ✓ Tuple: exact types preserved\nSequence{Tuple{Step{Cmd}, Step{Function}}}\n\nTuples enable the compiler to generate specialized code for each node.","category":"section"},{"location":"design/#2.-Multiple-Dispatch,-Not-Type-Checks","page":"Design","title":"2. Multiple Dispatch, Not Type Checks","text":"# ✗ Runtime type checking (type unstable)\nfunction run_node(node)\n    if node isa Step\n        # ...\n    elseif node isa Sequence\n        # ...\n    end\nend\n\n# ✓ Multiple dispatch (type stable)\nrun_node(step::Step, v) = execute(step)\nrun_node(seq::Sequence, v) = _run_sequence!([], seq.nodes, v)\nrun_node(par::Parallel, v) = _spawn_parallel(par.nodes, v)","category":"section"},{"location":"design/#3.-Tuple-Recursion","page":"Design","title":"3. Tuple Recursion","text":"Iterate tuples in a type-stable way:\n\n# Base case\n_run_sequence!(results, ::Tuple{}, v) = nothing\n\n# Recursive case\nfunction _run_sequence!(results, nodes::Tuple, v)\n    append!(results, run_node(first(nodes), v))\n    _run_sequence!(results, Base.tail(nodes), v)\nend\n\nThe compiler unrolls this into efficient, specialized code.","category":"section"},{"location":"design/#4.-Verbosity-as-Types","page":"Design","title":"4. Verbosity as Types","text":"struct Verbose end\nstruct Silent end\n\nprint_start(::Silent, ::Step) = nothing\nprint_start(::Verbose, s::Step) = println(\"▶ $(s.name)\")\n\nDead code elimination removes printing when verbose=false.","category":"section"},{"location":"design/#Performance-Characteristics","page":"Design","title":"Performance Characteristics","text":"Aspect Design Choice Benefit\nNode storage Tuples Full type info, inline storage\nDispatch Multiple dispatch Zero runtime type checks\nIteration Recursion Compiler unrolling\nOperators @inline Zero call overhead\nVerbosity Singleton types Dead code elimination","category":"section"},{"location":"development/#Extending-SimplePipelines","page":"Development","title":"Extending SimplePipelines","text":"SimplePipelines is designed for easy extension via Julia's multiple dispatch.","category":"section"},{"location":"development/#Architecture","page":"Development","title":"Architecture","text":"All nodes inherit from AbstractNode. To add custom behavior, define a new subtype and implement dispatch methods:\n\nAbstractNode\n    ├── Step{F}           # Leaf node\n    ├── Sequence{T}       # Sequential\n    ├── Parallel{T}       # Concurrent\n    ├── Retry{N}          # Retry on failure\n    ├── Fallback{A,B}     # Try A, else B\n    └── Branch{C,T,F}     # Conditional","category":"section"},{"location":"development/#Custom-Node-in-4-Steps","page":"Development","title":"Custom Node in 4 Steps","text":"using SimplePipelines\nimport SimplePipelines: AbstractNode, run_node, print_dag, count_steps, steps\n\n# 1. Define type (parametric for type stability)\nstruct Timeout{N<:AbstractNode} <: AbstractNode\n    node::N\n    seconds::Float64\nend\n\n# 2. Implement execution\nfunction run_node(t::Timeout, verbosity)\n    # ... timeout logic ...\n    return run_node(t.node, verbosity)\nend\n\n# 3. Implement visualization\nprint_dag(t::Timeout, indent::Int) = begin\n    println(\"  \"^indent, \"Timeout($(t.seconds)s):\")\n    print_dag(t.node, indent + 1)\nend\n\n# 4. Implement utilities\ncount_steps(t::Timeout) = count_steps(t.node)\nsteps(t::Timeout) = steps(t.node)","category":"section"},{"location":"development/#Custom-Operator","page":"Development","title":"Custom Operator","text":"import Base: ^\n\n# Node^3 means repeat 3 times\n^(node::AbstractNode, n::Int) = Repeat(node, n)","category":"section"},{"location":"development/#Custom-Step-Executor","page":"Development","title":"Custom Step Executor","text":"Extend execute for new work types:\n\nstruct HTTPGet\n    url::String\nend\n\nfunction SimplePipelines.execute(step::Step{HTTPGet})\n    start = time()\n    resp = HTTP.get(step.work.url)\n    return StepResult(step, resp.status == 200, time() - start, String(resp.body))\nend\n\n# Usage: Step(:fetch, HTTPGet(\"https://api.example.com\"))","category":"section"},{"location":"development/#Type-Stability-Rules","page":"Development","title":"Type Stability Rules","text":"Use parametric types: struct MyNode{T<:AbstractNode} not children::Vector{AbstractNode}\nUse tuple recursion for heterogeneous collections\nDispatch on types, don't check with isa","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Basic-Pipeline","page":"Examples","title":"Basic Pipeline","text":"A simple three-step workflow:\n\nusing SimplePipelines\n\ndownload = @step download = `curl -o data.txt https://example.com/data`\nprocess = @step process = `sort data.txt > sorted.txt`\nupload = @step upload = `scp sorted.txt server:/data/`\n\npipeline = download >> process >> upload\nrun_pipeline(pipeline)","category":"section"},{"location":"examples/#Parallel-Processing","page":"Examples","title":"Parallel Processing","text":"Process multiple files concurrently:\n\nusing SimplePipelines\n\n# Process each file independently\nfile_a = @step a = `gzip -k file_a.txt`\nfile_b = @step b = `gzip -k file_b.txt`\nfile_c = @step c = `gzip -k file_c.txt`\n\n# Archive all compressed files\narchive = @step archive = `tar -cvf archive.tar *.gz`\n\n# Files compress in parallel, then archive\npipeline = (file_a & file_b & file_c) >> archive\nrun_pipeline(pipeline)","category":"section"},{"location":"examples/#Julia-Computation","page":"Examples","title":"Julia Computation","text":"Mix Julia computation with external tools:\n\nusing SimplePipelines\n\n# Generate data in Julia\ngenerate = @step generate = () -> begin\n    data = rand(1000, 100)\n    writedlm(\"matrix.csv\", data, ',')\n    return \"Generated $(size(data)) matrix\"\nend\n\n# Process with external tool\nprocess = @step process = `./analyze matrix.csv -o stats.json`\n\n# Load and report in Julia\nreport = @step report = () -> begin\n    stats = JSON.parsefile(\"stats.json\")\n    println(\"Mean: $(stats[\"mean\"])\")\n    println(\"Std:  $(stats[\"std\"])\")\nend\n\npipeline = generate >> process >> report\nrun_pipeline(pipeline)","category":"section"},{"location":"examples/#Bioinformatics:-NGS-Alignment","page":"Examples","title":"Bioinformatics: NGS Alignment","text":"A typical next-generation sequencing alignment workflow:\n\nusing SimplePipelines\n\n# Quality control\nfastqc = @step fastqc = `fastqc -o qc/ reads.fq.gz`\n\n# Trim adapters\ntrim = @step trim = `trimmomatic SE reads.fq.gz trimmed.fq.gz ILLUMINACLIP:adapters.fa:2:30:10`\n\n# Align to reference\nalign = @step align = `bwa mem -t 8 reference.fa trimmed.fq.gz > aligned.sam`\n\n# Convert and sort\nsort_bam = @step sort = `samtools sort -@ 4 -o sorted.bam aligned.sam`\n\n# Index\nindex = @step index = `samtools index sorted.bam`\n\n# Full pipeline\npipeline = fastqc >> trim >> align >> sort_bam >> index\nrun_pipeline(Pipeline(pipeline, name=\"NGS Alignment\"))","category":"section"},{"location":"examples/#Bioinformatics:-Multi-Sample-Variant-Calling","page":"Examples","title":"Bioinformatics: Multi-Sample Variant Calling","text":"Process multiple samples in parallel, then joint-call variants:\n\nusing SimplePipelines\n\n# Per-sample processing function\nfunction sample_pipeline(name, fastq)\n    trim = Step(Symbol(\"trim_\", name), `trimmomatic SE $fastq $(name)_trimmed.fq.gz ...`)\n    align = Step(Symbol(\"align_\", name), `bwa mem ref.fa $(name)_trimmed.fq.gz > $(name).bam`)\n    sort = Step(Symbol(\"sort_\", name), `samtools sort -o $(name)_sorted.bam $(name).bam`)\n    return trim >> align >> sort\nend\n\n# Build per-sample pipelines\nsample_a = sample_pipeline(\"A\", \"sample_A.fq.gz\")\nsample_b = sample_pipeline(\"B\", \"sample_B.fq.gz\")\nsample_c = sample_pipeline(\"C\", \"sample_C.fq.gz\")\n\n# Joint variant calling (after all samples)\ncall = @step call = `bcftools mpileup -f ref.fa *_sorted.bam | bcftools call -mv -o variants.vcf`\n\n# Filter variants\nfilter_vcf = @step filter = `bcftools filter -e 'QUAL<20' variants.vcf > filtered.vcf`\n\n# Complete pipeline:\n#   (A & B & C) >> call >> filter\npipeline = (sample_a & sample_b & sample_c) >> call >> filter_vcf\n\nrun_pipeline(Pipeline(pipeline, name=\"Multi-Sample Variant Calling\"))","category":"section"},{"location":"examples/#Error-Handling-with-Retry-and-Fallback","page":"Examples","title":"Error Handling with Retry and Fallback","text":"Handle flaky operations gracefully:\n\nusing SimplePipelines\n\n# Retry a flaky API call up to 3 times\nfetch = @step fetch = `curl -f https://flaky-api.com/data -o data.json`\npipeline = Retry(fetch, 3, delay=2.0) >> process\n\n# If primary method fails, use fallback\nfast = @step fast = `fast-tool data.csv`\nslow = @step slow = `slow-tool data.csv`\npipeline = fast | slow\n\n# Combine: retry primary, then fallback\npipeline = Retry(fast, 3) | slow","category":"section"},{"location":"examples/#Conditional-Branching","page":"Examples","title":"Conditional Branching","text":"Choose execution path at runtime:\n\nusing SimplePipelines\n\n# Different processing based on file size\nsmall_pipeline = @step small = `quick-process data.csv`\nlarge_pipeline = @step decompress = `gunzip data.csv.gz` >> @step process = `parallel-process data.csv`\n\npipeline = Branch(\n    () -> filesize(\"data.csv\") < 100_000_000,  # < 100MB\n    small_pipeline,\n    large_pipeline\n)\n\n# Environment-based branching\ndebug_steps = @step debug = `./tool --verbose --debug data`\nprod_steps = @step prod = `./tool --quiet data`\n\npipeline = Branch(() -> get(ENV, \"DEBUG\", \"0\") == \"1\", debug_steps, prod_steps)","category":"section"},{"location":"examples/#Complex-DAG","page":"Examples","title":"Complex DAG","text":"A workflow with multiple parallel stages:\n\nusing SimplePipelines\n\n# Stage 1: Fetch from multiple sources (parallel)\nfetch_db = @step db = `curl -o db_data.json https://api.database.com/export`\nfetch_files = @step files = `rsync -av server:/data/ local_data/`\n\n# Stage 2: Transform each source (parallel, after fetch)\ntransform_db = @step transform_db = () -> transform_database(\"db_data.json\")\ntransform_files = @step transform_files = () -> transform_files(\"local_data/\")\n\n# Stage 3: Merge and analyze (sequential)\nmerge = @step merge = () -> merge_datasets(\"transformed_db.csv\", \"transformed_files.csv\")\nanalyze = @step analyze = `./analysis_tool merged.csv -o results/`\n\n# Stage 4: Generate outputs (parallel)\nreport = @step report = () -> generate_report(\"results/\")\narchive = @step archive = `tar -czvf results.tar.gz results/`\n\n# Build DAG\ndb_branch = fetch_db >> transform_db\nfiles_branch = fetch_files >> transform_files\n\npipeline = (db_branch & files_branch) >> merge >> analyze >> (report & archive)\n\nrun_pipeline(pipeline)","category":"section"},{"location":"examples/#Robust-Pipeline-(All-Features)","page":"Examples","title":"Robust Pipeline (All Features)","text":"Combining all operators for a production-ready workflow:\n\nusing SimplePipelines\n\n# Fetch with retry and fallback\nprimary_source = @step primary = `curl -f https://main-api.com/data -o data.json`\nbackup_source = @step backup = `curl -f https://backup-api.com/data -o data.json`\nfetch = Retry(primary_source, 3, delay=1.0) | backup_source\n\n# Conditional processing\nquick_process = @step quick = `jq '.items' data.json > processed.json`\nfull_process = @step parse = `./parse data.json` >> @step validate = `./validate parsed.json`\n\nprocess = Branch(\n    () -> filesize(\"data.json\") < 1_000_000,\n    quick_process,\n    full_process\n)\n\n# Parallel outputs\nreport = @step report = () -> generate_report(\"processed.json\")\nnotify = @step notify = `curl -X POST https://slack.com/webhook -d '{\"text\":\"Done\"}'`\n\n# Full pipeline\npipeline = fetch >> process >> (report & Retry(notify, 2))\n\nrun_pipeline(Pipeline(pipeline, name=\"Robust ETL\"))","category":"section"},{"location":"#SimplePipelines.jl","page":"Home","title":"SimplePipelines.jl","text":"Minimal, type-stable DAG pipelines for Julia","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"SimplePipelines.jl lets you define and execute directed acyclic graph (DAG) pipelines using intuitive operators.","category":"section"},{"location":"#Steps","page":"Home","title":"Steps","text":"Syntax Description\n@step name = \\cmd`` Shell command\n@step name = () -> expr Julia function","category":"section"},{"location":"#Operators","page":"Home","title":"Operators","text":"Operator Description Example\n>> Sequential a >> b >> c\n& Parallel a & b & c\n| Fallback a | b (b if a fails)\n^n Retry a^3 (up to 3 times)","category":"section"},{"location":"#Control-Flow","page":"Home","title":"Control Flow","text":"Function Description\nTimeout(a, secs) Fail if exceeds time\nBranch(cond, a, b) Conditional execution\nMap(f, items) Fan-out over collection\nReduce(f, a & b) Combine parallel outputs\nRetry(a, n, delay=d) Retry with delay","category":"section"},{"location":"#Execution-and-Results","page":"Home","title":"Execution & Results","text":"Function / Field Description\nrun_pipeline(p) Run, return results\nrun_pipeline(p, verbose=false) Run silently\nrun_pipeline(p, dry_run=true) Preview only\nresults[i].success Did step succeed?\nresults[i].duration Time in seconds\nresults[i].output Output or error","category":"section"},{"location":"#Utilities","page":"Home","title":"Utilities","text":"Function Description\nprint_dag(node) Visualize structure\ncount_steps(node) Count steps\nsteps(node) Get all steps","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"using SimplePipelines\n\n# Simple sequence\npipeline = `echo \"step 1\"` >> `echo \"step 2\"` >> `echo \"step 3\"`\nrun_pipeline(pipeline)\n\n# Parallel branches that merge\npipeline = (`process A` & `process B`) >> `merge`\nrun_pipeline(pipeline)","category":"section"},{"location":"#DAG-Patterns","page":"Home","title":"DAG Patterns","text":"","category":"section"},{"location":"#Diamond-(fork-join)","page":"Home","title":"Diamond (fork-join)","text":"       ┌── step_b ──┐\nstep_a─┤            ├── step_d\n       └── step_c ──┘\n\npipeline = step_a >> (step_b & step_c) >> step_d","category":"section"},{"location":"#Multi-stage-parallel","page":"Home","title":"Multi-stage parallel","text":"       ┌─ b ─┐     ┌─ e ─┐\n    a ─┤     ├─ d ─┤     ├─ g\n       └─ c ─┘     └─ f ─┘\n\npipeline = a >> (b & c) >> d >> (e & f) >> g","category":"section"},{"location":"#Independent-branches-merging","page":"Home","title":"Independent branches merging","text":"    ┌─ a ── b ─┐\n    │          │\n    ├─ c ── d ─┼── merge\n    │          │\n    └─ e ── f ─┘\n\nbranch1 = a >> b\nbranch2 = c >> d\nbranch3 = e >> f\n\npipeline = (branch1 & branch2 & branch3) >> merge","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Intuitive operators - >> sequence, & parallel, | fallback, ^ retry\nControl flow - Timeout, Branch, Map for complex workflows\nType-stable - Zero runtime type checks, full compile-time specialization\nMinimal overhead - @inline functions and tuple recursion\nComposable - All operators work together seamlessly\nUnified interface - Shell commands and Julia functions compose seamlessly","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"Pages = [\"tutorial.md\", \"examples.md\", \"api.md\", \"design.md\", \"development.md\"]\nDepth = 2","category":"section"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#Steps","page":"Tutorial","title":"Steps","text":"A Step is the basic unit of work—either a shell command or Julia function.","category":"section"},{"location":"tutorial/#Shell-Commands","page":"Tutorial","title":"Shell Commands","text":"# Direct command (anonymous step)\nstep = @step `samtools sort input.bam`\n\n# Named step\nstep = @step sort = `samtools sort input.bam`","category":"section"},{"location":"tutorial/#Julia-Functions","page":"Tutorial","title":"Julia Functions","text":"# Anonymous function step\nstep = @step () -> process_data()\n\n# Named function step\nstep = @step analyze = () -> run_analysis(\"data.csv\")","category":"section"},{"location":"tutorial/#File-Dependencies-(Optional)","page":"Tutorial","title":"File Dependencies (Optional)","text":"Track input/output files for validation:\n\n@step align(\"reads.fq\" => \"aligned.bam\") = `bwa mem ref.fa reads.fq > aligned.bam`","category":"section"},{"location":"tutorial/#Sequential-Execution:","page":"Tutorial","title":"Sequential Execution: >>","text":"The >> operator chains steps—each waits for the previous to complete:\n\n# Three steps in order\npipeline = step_a >> step_b >> step_c\n\n# Chain commands directly\npipeline = `download data.txt` >> `process data.txt` >> `upload results.txt`","category":"section"},{"location":"tutorial/#Parallel-Execution:-and","page":"Tutorial","title":"Parallel Execution: &","text":"The & operator groups steps to run concurrently:\n\n# Three steps in parallel\nparallel = step_a & step_b & step_c\n\n# Process samples in parallel, then merge\npipeline = (sample_1 & sample_2 & sample_3) >> merge_results","category":"section"},{"location":"tutorial/#Complex-DAGs","page":"Tutorial","title":"Complex DAGs","text":"Combine >> and & for arbitrary graphs.","category":"section"},{"location":"tutorial/#Diamond-Pattern","page":"Tutorial","title":"Diamond Pattern","text":"       ┌── analyze_a ──┐\n fetch─┤               ├── report\n       └── analyze_b ──┘\n\nfetch = @step fetch = `curl -o data.csv https://example.com/data`\nanalyze_a = @step a = `tool_a data.csv`\nanalyze_b = @step b = `tool_b data.csv`\nreport = @step report = () -> combine_results()\n\npipeline = fetch >> (analyze_a & analyze_b) >> report","category":"section"},{"location":"tutorial/#Multi-Stage-Parallel","page":"Tutorial","title":"Multi-Stage Parallel","text":"For graphs with multiple fork-join points, compose in stages:\n\n     ┌─ b ─┐     ┌─ e ─┐\n  a ─┤     ├─ d ─┤     ├─ g\n     └─ c ─┘     └─ f ─┘\n\na = @step a = `step_a`\nb = @step b = `step_b`\nc = @step c = `step_c`\nd = @step d = `step_d`\ne = @step e = `step_e`\nf = @step f = `step_f`\ng = @step g = `step_g`\n\npipeline = a >> (b & c) >> d >> (e & f) >> g","category":"section"},{"location":"tutorial/#Independent-Branches","page":"Tutorial","title":"Independent Branches","text":"Process independent pipelines in parallel, then merge:\n\n  ┌─ fetch_a >> process_a ─┐\n  │                        │\n  ├─ fetch_b >> process_b ─┼── merge\n  │                        │\n  └─ fetch_c >> process_c ─┘\n\nbranch_a = fetch_a >> process_a\nbranch_b = fetch_b >> process_b\nbranch_c = fetch_c >> process_c\n\npipeline = (branch_a & branch_b & branch_c) >> merge\n\nThis pattern is common for processing multiple samples/files independently before combining results.","category":"section"},{"location":"tutorial/#Fallback:","page":"Tutorial","title":"Fallback: |","text":"The | operator provides fallback behavior—if the primary fails, run the fallback:\n\n# If fast method fails, use slow method\npipeline = fast_method | slow_method\n\n# Chain multiple fallbacks\npipeline = method_a | method_b | method_c","category":"section"},{"location":"tutorial/#Retry:-or-Retry()","page":"Tutorial","title":"Retry: ^ or Retry()","text":"Retry a node up to N times on failure:\n\n# Using ^ operator (concise)\npipeline = flaky_api_call^3\n\n# Using Retry() with delay between attempts\npipeline = Retry(network_request, 5, delay=2.0)\n\n# Combine with fallback\npipeline = primary^3 | fallback","category":"section"},{"location":"tutorial/#Branch-(Conditional)","page":"Tutorial","title":"Branch (Conditional)","text":"Execute different branches based on a runtime condition:\n\n# Branch based on file size\npipeline = Branch(\n    () -> filesize(\"data.txt\") > 1_000_000,\n    large_file_pipeline,\n    small_file_pipeline\n)\n\n# Branch based on environment\npipeline = Branch(\n    () -> haskey(ENV, \"DEBUG\"),\n    debug_steps,\n    normal_steps\n)","category":"section"},{"location":"tutorial/#Timeout","page":"Tutorial","title":"Timeout","text":"Fail if a node exceeds a time limit:\n\n# 30 second timeout\npipeline = Timeout(long_running_step, 30.0)\n\n# Combine with retry and fallback\npipeline = Timeout(api_call, 5.0)^3 | backup","category":"section"},{"location":"tutorial/#Map-(Fan-out)","page":"Tutorial","title":"Map (Fan-out)","text":"Apply a function to each item, creating parallel steps:\n\n# Process files in parallel\nsamples = [\"sample_A\", \"sample_B\", \"sample_C\"]\npipeline = Map(samples) do s\n    Step(Symbol(\"process_\", s), `analyze $s.fastq`)\nend >> merge_results\n\n# Equivalent to:\n# (process_sample_A & process_sample_B & process_sample_C) >> merge_results","category":"section"},{"location":"tutorial/#Reduce-(Combine)","page":"Tutorial","title":"Reduce (Combine)","text":"Collect outputs from parallel steps and combine them:\n\n# Combine parallel outputs with a function\npipeline = Reduce(analyze_a & analyze_b) do outputs\n    join(outputs, \"\\n\")\nend\n\n# Using a named function\npipeline = Reduce(combine_results, step_a & step_b & step_c)\n\n# In a pipeline: fetch -> parallel analysis -> reduce -> report\npipeline = fetch >> Reduce(merge, analyze_a & analyze_b) >> report\n\nThe reducer function receives a Vector{String} of outputs from all successful parallel steps.","category":"section"},{"location":"tutorial/#Running-Pipelines","page":"Tutorial","title":"Running Pipelines","text":"# Basic execution\nresults = run_pipeline(pipeline)\n\n# Silent (no progress output)\nresults = run_pipeline(pipeline, verbose=false)\n\n# Dry run (preview structure)\nrun_pipeline(pipeline, dry_run=true)\n\n# Named pipeline\np = Pipeline(step_a >> step_b, name=\"My Workflow\")\nrun_pipeline(p)","category":"section"},{"location":"tutorial/#Checking-Results","page":"Tutorial","title":"Checking Results","text":"results = run_pipeline(pipeline)\n\nfor r in results\n    if r.success\n        println(\"$(r.step.name): completed in $(r.duration)s\")\n    else\n        println(\"$(r.step.name): FAILED - $(r.output)\")\n    end\nend\n\n# Check overall success\nall_ok = all(r -> r.success, results)","category":"section"},{"location":"tutorial/#Mixing-Shell-and-Julia","page":"Tutorial","title":"Mixing Shell and Julia","text":"Shell commands and Julia functions compose seamlessly:\n\n# Julia: prepare data\nprep = @step prep = () -> begin\n    data = load(\"raw.csv\")\n    cleaned = filter_invalid(data)\n    save(\"clean.csv\", cleaned)\nend\n\n# Shell: run external tool\nexternal = @step tool = `external_program clean.csv -o result.txt`\n\n# Julia: postprocess\npost = @step post = () -> parse_and_summarize(\"result.txt\")\n\npipeline = prep >> external >> post\nrun_pipeline(pipeline)","category":"section"},{"location":"tutorial/#Utilities","page":"Tutorial","title":"Utilities","text":"# Count steps in a pipeline\nn = count_steps(pipeline)\n\n# Get all steps as a vector\nall_steps = steps(pipeline)\n\n# Print DAG structure\nprint_dag(pipeline)","category":"section"}]
}
